\section{Filesystems}
Figure~\ref{fig:app_bench_ffs_read}, \ref{fig:app_bench_ffs_re_read}, and \ref{fig:app_bench_ffs_rnd_read} show that FFS performs poorly for Read operations with a small buffer size. Beginning at \SI{4}{\kilo\byte} buffer size, the performance in general goes up with the first few buffer sizes. This indicates that the overhead of the FFS read operation is high as the performance gets better when it can read fewer buffers. Overhead of the read operation includes, among other things, the time to fetch the image from Flickr if it is not in the cache, and decrypting the image which is required even if the image is cached. Further, it is expected that Re-Read should perform better than Read when the file size is small enough to fit in the cache. However, this is not an conclusion that can be drawn from the result. Looking at table~\ref{tbl:data_re-read_ffs}, it shows that there is no significant drop in performance for file sizes bigger than the \SI{5}{\mega\byte} cache file size limit. In fact, the performance is even better for the file sizes bigger than the cache file size limit than for the file sizes smaller than the cache file size limit for certain buffer sizes, such as \texttt{buffer size = 512kB} where the performance only increases for bigger file sizes. However, as Figure~\ref{fig:res_box_ffs} shows, the average performance of Re-Read is better than the average performance of Read for FFS. This supports the theory that the cache will increase the performance of the filesystem.

One interesting comparison is between the benchmark results of FFS and GCSF. Both filesystems are cloud-based FUSE filesystems dependant on an internet connection to their respective storage servers. Knowing that the benchmarking of the two filesystems were started at the same time, and the FFS benchmarking taking 41 minutes while the GCSF benchmarking took 20 minutes, it is already notable that GCSF is overall faster than FFS for the specified benchmarking test, using the defined file sizes and buffer sizes. The data presented in Section~\ref{sec:res_bench} and Appendix~\ref{app:bench_data} further confirms the conclusion that FFS is slower than GCSF. For instance, the Read test of GCSF performed in general better than the Read test of FFS. The median performance of the GCSF Read test is significantly better than the median performance of the FFS Read test. Looking at Figure~\ref{fig:res_box_ffs} and Figure~\ref{fig:res_box_gcsf} we can see that FFS produced much bigger spread of the values than GCSF did, especially for the Write, Re-Write, and Random Write tests. One outlier of the Write test of FFS performed at \SI[per-mode = symbol]{88671}{\kilo\byte\per\second}, namely for \texttt{file size = 8192kB, buffer size = 2048kB} as can be seen in Table~\ref{tbl:data_read_ffs}. This is better performance than any of the performance data points of GCSF for the Write, Re-Write, and Random Write tests.

Comparing the Fejk FFS (FFFS) benchmarking results against the APFS benchmarking results, we can compare the theoretical best performance of FFS against a general-purpose highly-used filesystem. In Table~\ref{tbl:data_read_fejk_ffs} and Table~\ref{tbl:data_read_apfs} we can see that the read operation perform almost similarly for FFFS and APFS, where APFS is in general faster than FFFS. However, for certain data points, such as \texttt{file size = 4096kB, buffer size = 4kB}, FFFS has higher throughput than APFS with \SI[per-mode = symbol]{2\,866\,270}{\kilo\byte\per\second} for FFFS and \SI[per-mode = symbol]{2\,402\,508}{\kilo\byte\per\second} for APFS. The cache of the filesystems can influence the performance of the read operation a lot. In the case of FFFS, the filesystem will cache the written data as long as its size is less the limit of \SI{5}{\mega\byte}. However, there is no significant difference between the FFFS performance of reading a file that fits in the FFFS cache, and one that does not. All files that are read in FFFS that are not in the FFFS cache are read from disk, which invokes at least one APFS read operation. While the APFS read operation called might not be called with the same buffer size as the read operation called by IOZone on FFFS, the performance of the FFFS read operation cannot exceed the APFS read operation. However, the similarity of the performance between the filesystem indicates that FFS implements fast read operations, and that the read operation performance of FFS depends to a high extent on the internet bandwidth and latency to the OWS, as well as the OWS's data processing performance.

While the values of the read operation for FFFS and APFS are comparable to each other, this is not the case for all tests. For instance is the write operation of FFFS much slower than the write operation of APFS as can be seen in Table~\ref{tbl:data_write_fejk_ffs} and Table~\ref{tbl:data_write_apfs}. The write operation of FFFS has about 2-3\% the performance of the write operation of APFS. The reason for this can be the fact that FFFS has to encrypt the data stored, including creating all the cryptographic variables such as the salt and the IV. While APFS is also an encrypted filesystem, it is possible that the cryptographic functions are much faster than for FFFS as they for instance can be run in kernel space, while FFFS is running in user space.

FFFS and GCSF are comparable in some tests, which is interesting as GCSF is dependent on an internet connection while FFFS is not. The median performance of the Read test on GCSF is slightly worse than the medium performance of the Read test on FFFS. Meanwhile, the median Re-Read performance of GCSF is better than the median Re-Read performance of FFFS. This indicates that GCSF implements a faster cache than FFFS. One reason might be that the FFFS cache stores the encrypted version of the image, meaning that before the data is read, the image must first be decrypted and decoded. As Google Drive provides the raw data of the file stored, it is possible that GCSF stores the raw data in its cache meaning that the data in the read operation can be returned faster. Re-Write and Random Write tests on FFFS outperform the same tests on GCSF. This is reasonable as the data written to GCSF must be uploaded to Google Drive, while the data written to FFFS is stored on the local disk. Uploading \SI{16}{\mega\byte} of data with the average (reference point) upload speed of \SI[per-mode = symbol]{92.95}{\mega\bit\per\second} would take about \SI{1.4}{\second}. Meanwhile, we can see in Table~\ref{tbl:data_write_apfs} that APFS can write \SI{16}{\mega\byte} of data as fast as \SI[per-mode = symbol]{1539559}{\kilo\byte\per\second} = \SI[per-mode = symbol]{1549.559}{\mega\byte\per\second}, meaning it would take about \SI{10}{\milli\second} to write the data. However, the data written by FFFS is larger than \SI{16}{\mega\byte} as the saved data by FFFS is inflated by encryption and PNG attributes.

It is easy to see, and it is not unexpected, that APFS outperforms FFS in performance. As a professional local filesystem, APFS will always have better performance than FFS. Further, like FFFS, the performance of FFS depends on the performance of APFS as the file which is uploaded to Flickr first needs to be saved on disk. This dependency could be removed, for instance by providing the temporary file to the FlickCURL library via a FUSE filesystem. Further, the median performance of the Re-Read test on FFS is about 72\% of the performance of the Re-Read test on APFS. With higher bandwidth and with another OWS, it is possible that FFS could increase its performance. In contrast, the median performance of the Re-Read test on FFS is about 76\% of the median performance of the same test on GCSF.