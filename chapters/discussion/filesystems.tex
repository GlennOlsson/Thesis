\section{Filesystems}
\label{sec:dis_fs}
Figure~\ref{fig:bench_ffs_read}, Figure~\ref{fig:bench_ffs_re_read}, and Figure~\ref{fig:bench_ffs_rnd_read} show that \gls{FFS} performs poorly for the Read tests with a small buffer size. Beginning at a \SI{4}{\kilo\byte} buffer size, the performance in general increases with the first few buffer sizes. This could be influenced by the overhead of the \gls{FFS} read operation. Overhead of the read operation includes, among other things, the time to fetch the images from Flickr if they are not in the filesystem cache, and decrypting the images which is required even if the images are in the filesystem cache. Further, it is expected that the \mbox{Re-Read} tests perform better than the Read tests when the file size is small enough to fit in the filesystem cache. This is also a conclusion that can be drawn from the result. However, looking at Figure~\ref{fig:bench_ffs_read}, and Figure~\ref{fig:bench_ffs_re_read}, we can see that the \mbox{Re-Read} tests overall performs better for file sizes bigger than the \SI{5}{\mega\byte} cache file size limit as well. Especially for file sizes \SI{32\,768}{\kilo\byte} to \SI{262\,144}{\kilo\byte}, the performance of the Read tests is in general very low compared to the \mbox{Re-Read} tests. As Figure~\ref{fig:res_box_read} and Figure~\ref{fig:res_box_reread} shows, the average performance of the \mbox{Re-Read} tests are more than double the average performance of the Read tests for \gls{FFS}. The reason why the \mbox{Re-Read} tests perform better than the Read test for files bigger than the filesystem cache size limit is most probably due to kernel caching of the files. The performance is higher than a usual internet connection bandwidth would be. An internet connection's bandwidth is often limited to a maximum of $\text{\SI[per-mode = symbol]{1}{\giga\bit\per\second}} = \text{\SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}}$ by the ISP, depending on the subscription. For the read operations of the \mbox{cloud-based} filesystems that exceed \SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}, the data has most probably been provided my some kind of cache. Note that this limit is much higher than the measured reference bandwidth presented in Section~\ref{sec:res_bench}, but we use this limit as a reference of a maximum bandwidth of the connection rather than as the actual bandwidth of the connection. All the data points for the \mbox{Re-Read} tests for \gls{FFS} and \gls{GCSF} exceed \SI[per-mode = symbol]{1\,000\,000}{\kilo\byte\per\second} indicating that all these files were served by some cache. For \gls{FFS}, the files over \SI[per-mode = symbol]{5}{\mega\byte} were most probably served from the kernel cache as they cannot fit in the filesystem cache. As the performances differ significantly between the filesystems, it is possible that not all data was provided by the kernel cache, but that some data was provided by internal filesystem caches. Otherwise, the data could be more similar, assuming the kernel cache has the same performance for all filesystems tested. Many data points of the Read tests for \gls{FFS} and \gls{GCSF} also exceed \SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}, indicating that some data returned when running this test were also served by some cache. This data could be saved in the cache after the \mbox{Re-Write} tests which precedes the Read tests. The data could also be cached after the first Read test of the file which could explain the increasing performance for the smaller buffer sizes in the beginning of the tests. The biggest file sizes have lower performance than the other file sizes for the Read tests of \gls{FFS} and \gls{FFFS}, although the performance is still higher than the maximum bandwidth reference point of \SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}, indicating that the kernel cache has worse performance for bigger files.

% TODO: No, IOZone closes the file. Remove. Can we keep some discussion?
Other than the data being provided by the kernel cache, it is also possible that IOZone does not close the file before it is read again, meaning that the file can be kept in memory for \gls{FFS} and \gls{FFFS} as they cache open files that have been read. IOZone does not specify when the file is closed, however it could be assumed that the files are closed between tests. If the file would not be closed after a write test, the write tests of \gls{FFS} should have similar performance for the write tests as the write tests of \gls{FFFS} as neither of the two filesystems save the data in their storage medium until the file has been closed. As the write tests of \gls{FFS} and \gls{FFFS} are not similar it is improbable that the file is not closed after the write tests. The \mbox{Re-Read} test is performed after the Read test, and as the \mbox{Re-Read} test is significantly faster than the Read test for both \mbox{FFS} and \gls{FFFS}, even for files bigger than the cache limit of the filesystems, it is possible that the files are not closed between the two tests. However, if this was the case, the \mbox{Re-Read} tests would probably be more similar for \gls{FFS} and \gls{FFFS} than the results show, as the data could be served just as fast by both filesystems. The theory that the kernel cache is providing a better performance of the \mbox{Re-Read} is more probable as the tests are less consistent between the two filesystems. If the kernel would not have the requested data in its cache, the filesystem would have to be called. This could be the explanation why the filesystem performances differ significantly between the two filesystems. Cache misses in the kernel cache requires \gls{FFS} and \gls{FFFS} to get the data from their cache if it is present there, or from their respective storage medium. This introduces performance differences between the filesystems. Cache misses in the filesystem cache is less probably as there are no more files being saved, meaning that the files will not be removed from the cache.

While the \mbox{Re-Read} and Random Read tests increases in performance for the first buffer sizes for \gls{FFS}, the performance also decreases eventually. Looking at the data presented in Figure~\ref{fig:bench_ffs_re_read} and Figure~\ref{fig:bench_ffs_rnd_read}, the buffer sizes \SI[per-mode = symbol]{4\,096}{\kilo\byte} to \SI[per-mode = symbol]{16\,384}{\kilo\byte} have, in general, lower performance than the buffer sizes \SI[per-mode = symbol]{256}{\kilo\byte} to \SI[per-mode = symbol]{2\,048}{\kilo\byte}, for the same file size. This indicates that the optimal buffer size for the \gls{FFS} read operations on previously read files is not the biggest possible buffer size, but rather around \SI[per-mode = symbol]{512}{\kilo\byte}, depending on the file size. The biggest file size has its best performance for a buffer size of \SI[per-mode = symbol]{512}{\kilo\byte}. Looking at the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size, it peaks for both the \mbox{Re-Read} test and the Random read test at \texttt{buffer size = 128\,kB}, and its performance for that buffer size for both tests are higher than any other performance of any file size or buffer size in both tests, for the filesystem. This is interesting because the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size does not always outperform the other file sizes. Looking at the \mbox{Re-Read} test, the \SI[per-mode = symbol]{262\,144}{\kilo\byte} has the best performance for seven of the 13 tests while the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size has the best performance for five buffer sizes, namely the first one, the last three and for \texttt{buffer size = 128\,kB}. The \SI[per-mode = symbol]{16\,384}{\kilo\byte} file size has the best performance of the test once, for \texttt{buffer size = 1\,024\,kB}. Considering how fast the operations actually are, it is possible to understand why the values can fluctuate. For instance, the \mbox{Re-Read} test for \gls{FFS} using \texttt{file size = 16\,384\,kB, buffer size = 1\,024\,kB} has a performance of \SI[per-mode = symbol]{7\,599\,353}{\kilo\byte\per\second}. Transferring \SI[per-mode = symbol]{16\,284}{\kilo\byte} at \SI[per-mode = symbol]{7\,599\,353}{\kilo\byte\per\second} takes \SI[per-mode = symbol]{2.14}{\milli\second}. If it would take 10\% more time, the performance would instead be under \SI[per-mode = symbol]{7\,000\,000}{\kilo\byte\per\second}, meaning that the \SI[per-mode = symbol]{262\,144}{\kilo\byte} file size would have better performance for the same buffer size. However, if it instead would take 10\% less time to perform the \texttt{file size = 16\,384\,kB, buffer size = 1\,024\,kB} \mbox{Re-Read} test, it would reach a performance of \SI[per-mode = symbol]{8\,443\,726}{\kilo\byte\per\second} which would be the highest performance of the test on \gls{FFS} of all file sizes and buffer sizes. Small time differences can significantly affect the performance of the tests. The time of the filesystem operations can be fluctuated by process scheduling and the performance of the kernel cache, among other things.

The performance of the write operations of \gls{FFS} are highly influenced by the file size. As shown in Figure~\ref{fig:bench_ffs_write}, Figure~\ref{fig:bench_ffs_re_write}, and Figure~\ref{fig:bench_ffs_rnd_write}, bigger file sizes implicate better performance for the write operations, generally. The best performing file size was most often the largest file size, \SI[per-mode = symbol]{262\,144}{\kilo\byte}. Furthermore, the biggest file size of the tests perform better for the Write test than for the \mbox{Re-Write} and Random Write test. This is interesting as the Write test includes the overhead of creating the files before writing to them, which \mbox{Re-Write} and Random Write does not.

One interesting comparison is between the benchmark results of \gls{FFS} and \gls{GCSF}. Both filesystems are \mbox{cloud-based} \gls{FUSE} filesystems dependent on an internet connection to their respective storage servers. Looking at the box plots in Section~\ref{sec:res_bench}, we can see that \gls{GCSF} outperforms \gls{FFS} in both average performance and median performance for all tests. However, \gls{GCSF} does not have any data for the biggest file size while \gls{FFS} has data for it. Looking at Figure~\ref{fig:bench_ffs_re_read} and Figure~\ref{fig:bench_gcsf_re_read}, we can see that \gls{FFS} performs better than \gls{GCSF} for many of the bigger file sizes for the \mbox{Re-Read} test. It is possible that \gls{FFS} would perform better than \gls{GCSF} for the \SI{262\,144}{\kilo\byte} file size test if \gls{GCSF} could run the test. However, even if that would be the case, it is also possible that \gls{GCSF} would still perform better overall. One reason that \gls{GCSF} generally outperforms \gls{FFS} could be because the \gls{FFS} cache stores the encrypted version of the image, meaning that before the data is read, the image must first be decrypted and decoded. As Google Drive provides the raw data of the file stored, \gls{GCSF} can store the raw data in its cache meaning that the data in the read operation can be returned faster. If Google Drive caches the raw file data as well, it does not have to decrypt the data when serving it to \gls{GCSF}. \gls{GCSF} also outperforms \gls{FFS} in all the write tests. The reason could be that \gls{GCSF} does not have to encrypt the data nor encode it as images. This could save significant computation time. The average (reference point) bandwidth measured when the two filesystem benchmarks were run are similar, indicating that there was not a big difference in the internet connection to the measurement servers. However, as this does not measure the actual internet connection to the \gls{OWS}, the actual bandwidth of the filesystem could be different from this value. However, even assuming that the bandwidth of the internet connections of \gls{GCSF} and \gls{FFS} are equal, \gls{GCSF} can still benefit from fewer REST \gls{API} calls. As Google Drive is a filesystem, the inode table of the filesystem, or how ever the filesystem is organized, can be stored on the service without exposing it to the user. For instance, assuming that Google Drive uses an inode table like \gls{FFS}, the inode table would never have to be downloaded and uploaded by \gls{GCSF}. By simply uploading a file and specifying its path and filename, the inode table does not have to be modified by the user but can be handled by Google Drive in the background, potentially after the request has completed requiring less time for the file upload request. Meanwhile, \gls{FFS} has to upload the inode table after every file modification. Additionally, the old version of the file and inode table must be removed. This requires \gls{FFS} to perform at least four requests for all modifications:
\needspace{5\baselineskip}
\begin{itemize}
	\item Upload a new image with the new file content,
	\item Upload a new image with the new inode table content,
	\item Remove the old file, and,
	\item Remove the old inode table
\end{itemize}
Although, removing the old images is performed on another thread and does not affect the filesystem operation time. However, it can affect the congestion of the bandwidth to the \gls{OWS} if it is performed at the same time as another request, such as a file upload. Meanwhile, uploading a modified file to Google Drive requires one \gls{API} call using the file's ID\,\cite{FilesUpdateDrive2022} which will perform the same functionality as the four requests required for \gls{FFS}. The ID of the file could be stored locally in memory by \gls{GCSF} to be able to serve file ID's quickly, but this data structure does not have to be uploaded to Google Drive. Furthermore, when downloading a file, parts of the file can be downloaded rather than the full file\,\cite{googleDownloadFilesDrive2022}. This can reduce the time as the full file does not have to be downloaded every time a file is read. Even if we could download parts of a file from Flickr, it would not make sense for \gls{FFS} as we need the full file content to decode the image pixel data as the encrypted cipher text. However, by downloading the full image when it is read, subsequent reads are faster which is beneficial for the benchmarking tests as it often needs to read the file multiple times at different offsets. Google Drive has 800 million daily users\,\cite{lardinoisGoogleUpdatesDrive2017} versus Flickr's 60 million monthly users\,\cite{campbellFlickrStatistics20222022}, which means Google Drive is a much bigger service. This could mean that Google Drive has better infrastructure and that they can process the uploaded data faster and serve the data for download faster than Flickr can.

Certain data points in the graphs presented in Section~\ref{sec:res_bench} are outliers from groups of data points. For instance, looking at the Random Write test for \gls{GCSF} in Figure~\ref{fig:bench_gcsf_re_read} for \texttt{file size = 65\,536\,kB, buffer size = 16\,384\,kB}, the test data point has significantly lower performance than the other data points for the other buffer sizes in the same test with the same file size. There are many possible reasons behind this drop in performance. One reason could be a slow internet connection to Google Drive or slower data processing by Google Drive, for instance, due to a higher load of other user requests to the service at the point in time when the specific data point was benchmarked. Due to the \gls{OWS} being an external service that other users use at the same time, it is always possible that the \gls{OWS} of the \mbox{cloud-based} filesystems experience a high user demand at any time. Another reason for the data outlier can also be because the operating system scheduler scheduled the \gls{GCSF} process unfavorable at that time. This is an especially possible reason for the outlier data points for the \mbox{non-cloud-based} filesystems \gls{FFFS} and \gls{APFS} as they do not rely on an internet connection or an \gls{OWS}. For instance, Figure~\ref{fig:bench_apfs_rnd_read} shows two outliers for \gls{APFS} for \texttt{file size = 131\,072\,kB}, namely \texttt{buffer size = 4\,096\,kB} and \texttt{buffer size = 8\,192\,kB}. They could also have lower performance due to disk scheduling and cache management. The files in the \gls{APFS} cache could be invalidated and removed if other processes are reading other files from the disk at the same time, resulting in a cache miss of the benchmark file.

Other outlier data points have much higher performance than the other data points in a test. For instance, looking at the Write test for \gls{FFFS} in Figure~\ref{fig:bench_fffs_write}, there are data points for \texttt{file size = 8\,192\,kB}, \texttt{file size = 16\,384\,kB}, and \texttt{file size = 32\,768\,kB} that have much higher performance than the other data points. While most data points are approximately between \SI[per-mode = symbol]{6\,500}{\kilo\byte\per\second} to \SI[per-mode = symbol]{8\,300}{\kilo\byte\per\second}, two of these file sizes have one outlier, and one has two outliers, of approximately \SI[per-mode = symbol]{100\,000}{\kilo\byte\per\second}. Outliers can also be seen in Figure~\ref{fig:bench_fffs_re_write} and Figure~\ref{fig:bench_fffs_rnd_write} for the \mbox{Re-Write} and Random Write tests on \gls{FFFS}. \gls{FFFS} is not a cloud-based filesystem and is therefore not affected by, for instance, a fluctuating bandwidth of an internet connection. Rather, this is possibly the result of favorable process scheduling and disk operation scheduling. As both file sizes exceed the cache limit of \gls{FFFS}, the cache of the filesystem should not affect the value. However, it is also a possibility that IOZone did not close the file after the test or that the \texttt{close} operation was not performed correctly after test, which would mean that the data would not be written to the storage medium resulting in shorter execution time, leading to a higher performance for the write tests. The data of the open file would also be stored in the memory of \gls{FFFS}. Therefore, if there was a missed \texttt{close} operation, a subsequent read test should also be an outlier data point with higher performance than the other data points. Considering the outlier for the \mbox{Re-Write} test for \texttt{file size = 16\,384\,kB, buffer size = 8\,192\,kB}, and looking at the subsequent Read test for \texttt{file size = 16\,384\,kB, buffer size = 8\,192\,kB}, the data point for the Read is not an outlier. This indicates that the outlier in the \mbox{Re-Write} test probably is not due to a missed \texttt{close} call. Rather, it is possible that the higher performing \mbox{Re-Write} tests outliers are due to process scheduling. 

Comparing \gls{FFFS} benchmarking results against the \gls{APFS} benchmarking results, we can compare the theoretical best performance of \gls{FFS} against a \mbox{general-purpose} \mbox{widely-used} filesystem. Furthermore, we can compare \gls{FFFS} against the underlying filesystem in which it is storing its data. In Figure~\ref{fig:bench_fffs_read} and Figure~\ref{fig:bench_apfs_read} we can see that the read operation perform similarly for \gls{FFFS} and \gls{APFS}, where \gls{APFS} is in general faster than \gls{FFFS}. However, for certain data points, such as \texttt{file size = 131\,072\,kB, buffer size = 256\,kB}, \gls{FFFS} has a higher performance than \gls{APFS} with \SI[per-mode = symbol]{7\,525\,973}{\kilo\byte\per\second} for \gls{FFFS} and \SI[per-mode = symbol]{5\,927\,107}{\kilo\byte\per\second} for \gls{APFS}. As the file size is bigger than \SI{5}{\mega\byte} it was not stored in the cache of \gls{FFFS}. The reason why \gls{FFFS} outperformed \gls{APFS} at this data point is could be due to process scheduling or because the buffer size used by the test is less efficient for \gls{APFS} than the one called on \gls{APFS} by \gls{FFFS}. The buffer size used by \gls{FFFS} to read the file from \gls{APFS} is not certain to be the same as \gls{FFFS} was called with. The buffer size used by \gls{FFFS} on \gls{APFS} is set by the implementation of \texttt{basic\_filebuf}\,\cite{cppreference.comStdBasicFilebuf2020}. This has been found through testing to be the same size as the image written by \gls{FFFS} to \gls{APFS} if it is less or equal than \SI[per-mode = symbol]{65\,536}{\byte}, otherwise multiple buffers are used where the biggest ones are \SI[per-mode = symbol]{65\,536}{\byte}. It is also possible that the data was provided by the kernel cache to \gls{FFFS} while the same test for \gls{APFS} did not get the data from the kernel cache. Another possibility is that \gls{FFFS} read the data from \gls{APFS}, but it was provided by a cache of \gls{APFS} rather than being read from the \gls{APFS} storage device, while the same test for \gls{APFS} read the data from the storage device.

The cache of a filesystem can generally influence the performance of the read operations. However, there is no significant drop in performance for \gls{FFFS} when reading a file that fits in the \gls{FFFS} cache, and one that does not. In fact, the performance of the Read test for \texttt{file size = 8\,192} is in general better than for \texttt{file size = 4\,096} and \texttt{file size = 2\,048} on \gls{FFFS}, even though the \SI{8\,192}{\kilo\byte} file cannot fit in the \gls{FFFS} cache while the other file sizes can, as long as the encrypted data and the PNG attributes do not make the file bigger than \SI[per-mode = symbol]{5}{\mega\byte}. The reason why these files can be provided fast is most probably due to the kernel caching of the files. However, the performance of the \texttt{file size = 262\,144\,kB} compared to many of the other file sizes is much lower, indicating that these files were not, at least entirely, cached by the kernel, possibly due to size constraint. As the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size has similar performance to the \SI[per-mode = symbol]{262\,144}{\kilo\byte} file size for some of the \gls{FFFS} Read tests buffer sizes, but significantly higher performance for other buffer sizes, it is possible that the kernel cache size limit is around \SI[per-mode = symbol]{131\,072}{\kilo\byte}. However, as it seems to fluctuate, the size limit might be dependent on factors such as the available memory of the system at the time. When the memory usage is high, for instance, by other processes running at the same time, it is possible that the data stored in the kernel cache for \gls{FFFS} is removed, which decreases the performance for certain tests. 

The only implementation difference between \gls{FFS} and \gls{FFFS} is that \gls{FFS} stores the produced images on Flickr while \gls{FFFS} stores the produced images on the local filesystem. Therefore, the time difference of an \gls{FFS} operation compared to an \gls{FFFS} operation should only depend on the internet connection to Flickr and how fast Flickr can process the requests. For instance, looking at the Write tests for the two filesystems, \gls{FFFS} outperforms \gls{FFS} significantly. Looking at one file size, for instance, \texttt{file size = 8\,192} \gls{FFFS} has an average performance of approximately \SI[per-mode = symbol]{15\,800}{\kilo\byte\per\second} and \gls{FFS} has an average performance of approximately \SI[per-mode = symbol]{1\,050}{\kilo\byte\per\second} for the Write test. This means that the test took on average \SI{518}{\milli\second} for \gls{FFFS} and \SI{7\,802}{\milli\second} for \gls{FFS}. The same test for the same file size for \gls{APFS} had an average performance of \SI[per-mode = symbol]{612\,000}{\kilo\byte\per\second}, meaning it took on average \SI[per-mode = symbol]{13}{\milli\second} for \gls{APFS} to save the \SI[per-mode = symbol]{8\,192}{\kilo\byte} file. The time the test takes for \gls{FFFS} is the same as the write operation overhead plus the time \gls{APFS} takes to save the file. Subtracting the \gls{APFS} write time from the test time of \gls{FFFS}, we get the average overhead of the Write test for both \gls{FFS} and \gls{FFFS} as they have the same overhead. This overhead includes encrypting the data and encoding the result as an image. The average overhead time of the Write test for \gls{FFS} and \gls{FFFS} is therefore \SI[per-mode = symbol]{505}{\milli\second}, meaning that the request and the request's overhead by \gls{FFS} took on average $7\,802 - 505 =$ \SI{7\,297}{\milli\second} which is approximately 94\% of the computation time for this test. Assuming the upload bandwidth to Flickr is the same as the measured reference bandwidth of \SI[per-mode = symbol]{92.95}{\mega\byte\per\second}, uploading \SI[per-mode = symbol]{8\,192}{\kilo\byte} would take \SI[per-mode = symbol]{705}{\milli\second}. This means that the remaining \SI[per-mode = symbol]{6\,592}{\milli\second} were used for request overhead, such as preparing for the request, waiting for Flickr to process the data, and receiving the response from Flickr, including the post ID. Preparing the request includes first saving the image to the local filesystem, and then reading it when uploading it. Assuming the file was saved with the same \gls{APFS} performance as above, it would take \SI[per-mode = symbol]{13}{\milli\second} to save the file. Reading the file with the \gls{APFS} average Read test performance for a \SI[per-mode = symbol]{8\,192}{\kilo\byte} file of \SI[per-mode = symbol]{5\,505\,000}{\kilo\byte\per\second} would take \SI[per-mode = symbol]{1.5}{\milli\second}. The remaining \SI[per-mode = symbol]{6\,577.5}{\milli\second} are used for creating the request, receiving the request response, and for Flickr to process the data. The most significant time consumption of these three tasks is most probably the data processing of Flickr. This indicates that with faster data processing by Flickr, \gls{FFS} could potentially be faster. Furthermore, it indicates that the bandwidth of the internet connection to the \gls{OWS} is not the most important factor of the performance of \gls{FFS}. Even if uploading the file over the internet to the \gls{OWS} would be instant, it would reduce the file operation time by less than 10\%. To improve the filesystem operation performance, using a \gls{OWS} that can process the data faster is of more importance. For instance, if the \gls{OWS} would process the data in the background and instantly return, the write operation performance could be reduced. This could mean, however, that the image is not seen on the \gls{OWS} instantly which could mean that it is not possible to download the file instantly after it has been uploaded. The calculations above assume that the bandwidth to Flickr was approximately the same as the bandwidth to the measurement servers. It is possible that the bandwidth to Flickr was much lower, which would mean that the bandwidth has more impact of the filesystem operation time. Future work includes using multiple \gls{OWS}s to compare their performances, as well as measuring the individual filesystem's bandwidth for more precise calculations.

While the values of the read operation for \gls{FFFS} and \gls{APFS} are comparable to each other, this is not the case for all tests. For instance, the write operation of \gls{FFFS} is much slower than the write operation of \gls{APFS}. The write operation performance average of \gls{FFFS} is approximately 1.5\% of the average performance of the write operation on \gls{APFS}. While it is understandable that the \gls{FFFS} write performance is lower than the \gls{APFS} write performance as all data written to \gls{FFFS} must also be written to \gls{APFS}, the difference in performance is significant. The reason for this could be the fact that \gls{FFFS} has to encrypt the data stored, including creating all the cryptographic variables such as the salt and the \gls{IV}. Using the calculations above, we estimate that the average overhead of an \gls{FFFS} write operation for a \SI[per-mode = symbol]{8\,192}{\kilo\byte} file is \SI[per-mode = symbol]{505}{\milli\second} out of the total \SI{518}{\milli\second} for the filesystem operation, which is 97\% of the time of the operation. Assuming that this number is similar for other file sizes, it is a very significant portion of the execution time. While \gls{APFS} is also an encrypted filesystem, it is possible that the filesystem prepares the cryptographic variables before they are needed. For instance, the next cryptographic key and its salt could be derived while the filesystem is idle as it does not depend on the content of the stored data. It is also possible that \gls{APFS} is using multiple threads to write the data which could speed up the operation. For instance, it is possible that \gls{FFFS} requires multiple \texttt{write} calls to the underlying \gls{APFS} filesystem. If each \texttt{write} call to \gls{APFS} is computed on a different thread, the multiple \texttt{write} calls could possibly be completed faster rather than if they were completed sequentially.

\gls{FFFS} and \gls{GCSF} are comparable in some tests, which is interesting as \gls{GCSF} is dependent on an internet connection and an \gls{OWS} while \gls{FFFS} is not. The median performance of the \mbox{Re-Read} test on \gls{GCSF} is slightly worse than the medium performance of the \mbox{Re-Read} test on \gls{FFFS}. Meanwhile, the median Read performance of \gls{GCSF} is significantly less than the median Read performance of \gls{FFFS}. This indicates that a lot of the data served by \gls{FFFS} and \gls{GCSF} was served by the kernel cache of the \gls{FUSE} filesystems. Furthermore, it indicates that \gls{FFFS} is faster than \gls{GCSF} for files not in the kernel cache. However, as the Read tests performance is higher than a reasonable internet bandwidth, it is probable that the data from \gls{GCSF} is still served from an internal cache rather than the data being fetched from Google Drive. As the Read tests performances are higher for \gls{FFFS} than for \gls{GCSF}, it indicates that \gls{FFFS} has a faster internal cache than \gls{GCSF}, or that the kernel cache was used for the \gls{FFFS} Read tests while it was not used for \gls{GCSF}.

The Write, \mbox{Re-Write}, and Random Write tests on \gls{FFFS} outperform the same tests on \gls{GCSF}. This is reasonable as the data written to \gls{GCSF} must be uploaded to Google Drive, while the data written to \gls{FFFS} is stored on the local disk. Uploading \SI{16}{\mega\byte} of data with the average (reference point) upload speed of \SI[per-mode = symbol]{91.83}{\mega\bit\per\second} would take approximately \SI{1.4}{\second}. Meanwhile, we can see in Figure~\ref{fig:bench_apfs_write} that \gls{APFS} can write \SI{16}{\mega\byte} of data as fast as \SI[per-mode = symbol]{6\,921\,222}{\kilo\byte\per\second} = \SI[per-mode = symbol]{6\,921.2}{\mega\byte\per\second}, meaning it would take approximately \SI{2}{\milli\second} to write the data. Meanwhile, the maximum Write performance of \gls{FFFS} is \SI[per-mode = symbol]{101\,677}{\kilo\byte\per\second}, meaning that \gls{FFFS} can write \SI{16}{\mega\byte} in approximately \SI{157}{\milli\second}. With this data, we can see that \gls{FFFS} can write the \SI{16}{\mega\byte} of data approximately 7\,800\% slower than \gls{APFS} can, and that \gls{GCSF} can write the \SI{16}{\mega\byte} of data approximately 800\% slower than \gls{FFFS} or approximately 70\,000\% slower than \gls{APFS}. 

It is easy to see, and it is not unexpected, that \gls{APFS} outperforms \gls{FFS} in performance. As a professional local filesystem, \gls{APFS} will always have better performance than FFS. Further, like \gls{FFFS}, the performance of \gls{FFS} depends on the performance of \gls{APFS} as the file which is uploaded to Flickr first needs to be saved on disk. This dependency could be removed, for instance, by providing the temporary file to the FlickCURL library via a \gls{FUSE} filesystem. Further, the median performance of the \mbox{Re-Read} test on \gls{FFS} is approximately 72\% of the performance of the \mbox{Re-Read} test on \gls{APFS}. This is probably high for many of the tests because of the kernel cache of \gls{FFS}, but as it differs significantly from \gls{APFS}, \gls{FFS} calls are probably also performed. With higher bandwidth and with another \gls{OWS} with faster data processing, it is possible that \gls{FFS} could increase its read performance. In contrast, the median performance of the \mbox{Re-Read} test on \gls{FFS} is approximately 76\% of the median performance of the same test on \gls{GCSF}.

Generally, the figures of the benchmark data presented in Section~\ref{sec:res_bench} have visibly similar patterns for the Write tests and similar patterns the Read tests, per filesystem. For instance, the patterns of the \gls{FFS} Read, \mbox{Re-Read}, and Random Read tests follow a similar pattern with a similar curve of the data points, while the Write, \mbox{Re-Write}, and Random Write tests follow another distinct pattern. The Read, \mbox{Re-Read}, and Random Read figures of \gls{GCSF} follow another distinct pattern, as well as the three Write test data follow a fourth distinct pattern. The three write test patterns of \gls{FFS} are dissimilar to the patterns of the \gls{FFFS} write tests even though both filesystems are implemented very similarly, other than the storage medium. However, some patters are similar even though they are from different filesystem, for instance, the Read tests of \gls{FFS} and \gls{FFFS}. In Figure~\ref{fig:bench_ffs_read} and Figure~\ref{fig:bench_fffs_read}, the Read test shows that certain file size data points are found on the lower spectrum of the plot, while the other file sizes follow a somewhat similar curve for both filesystems. However, the pattern of the \mbox{Re-Read} test data of \gls{FFS} and \gls{FFFS} differ significantly. It might be possible to use these distinct patterns to create a fingerprint of a filesystem. This could be used, for instance, to identify a filesystem based on its performance when the filesystem is unknown. This could be useful when the filesystem is masked by an overlaying filesystem such as Cryptfs. However, by benchmarking the overlying filesystem, the pattern of the underlying filesystem might be lost. Analyzing if it is possible to identify filesystems based on benchmark fingerprints and identifying underlying filesystems using this technique is part of future work.

The bandwidth reference measurements are performed every five minutes while \gls{FFS} and \gls{GCSF} are benchmarked. Each bandwidth measurement test takes about \SI[per-mode = symbol]{20}{\second} each. During this time, the measurement tool is uploading and downloading data to the measurement servers. This can cause congestion of the internet connections of the filesystems which could reduce their performance. Bredbandskollen estimates that each measurement requires approximately as half as many bytes as the measurement shows in bits per second\,\cite{internetstiftelsenMerOmBredbandskollen}. The average bandwidth measurements while the two filesystems were running were both approximately \SI[per-mode = symbol]{90}{\mega\bit\per\second} for both the upload and download speeds. This means that the measurements consumed about \SI[per-mode = symbol]{45}{\mega\byte} per measurement. This is bigger than most file sizes tested when benchmarking. Furthermore, background processes running at the same time as the filesystem benchmarks  and using internet connections can further congest the internet connection of the filesystems. If we could measure each filesystem's individual internet connection's bandwidth, calculations regard the impact of the bandwidth could be more precise. This is part of future work.



% TODO: 
%
% - Compare difference between kernel cache performance
%	- all over 125 000 is kernel cache for gcsf and ffs read operations
% 	- how much does it fluctuate?
%	- create figure, scatter all points
%
% - Not super fair to use kernel cache/writing and asking for data directly
% 	- more normal use case could be writing data once in a while and asking for it once in a while
%	- Write tests are more fair though
%
%	- Critically analyze method of collecting data
%		- Only running once, using kernel RAM etc.