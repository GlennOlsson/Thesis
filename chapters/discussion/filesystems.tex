\section{Filesystems}
% TODO: Are you able to say something about where the bulk of time is the overhead is going? Is it purely network overhead? If so, how do network conditions affect performance?
\label{sec:dis_fs}

The read test of the filesystems have significantly lower performance when the \gls{UBC} is disabled compared to when it is enabled. This is expected as disabling the cache provided by the operating system kernel requires the process running the filesystem to be called with the arguments. The data read during the read test is the same data written to the file during the write test, so the data can be cached and reported back without accessing the underlying storage device. The write test is not influenced as much by the state of the \gls{UBC} for most of the filesystems. \gls{FFS}, \gls{GCSF}, and \gls{FFFS} have similar performances for when the \gls{UBC} is enabled and disabled for the write test. \gls{APFS} has a slightly lower performance when the \gls{UBC} is disabled compared to when it is enabled. It is understandable that the \gls{UBC} does not influence the write operation performance as significantly as it does influence the read performance. The slight difference for \gls{APFS} could be outliers due to, for instance, difference in memory usage and CPU usage by other processes during the benchmarks. This could be confirmed by running more benchmarks and increasing the confidence level of the data. There could also be benefits with an enabled \gls{UBC} for the write operation that makes \gls{APFS} perform better with the \gls{UBC} enabled.

\gls{FFS} and \gls{FFFS} perform similarly for read test when the \gls{UBC} is disabled, with \gls{FFFS} performing slightly better. The spread of values by \gls{FFS} is greater than the spread of \gls{FFFS}, and the lowest 31 performance data points of \gls{FFS} are worse than the lowest performance data point of \gls{FFFS}. The data of \gls{FFS} with the \gls{UBC} disabled does not include benchmarking of the biggest file size. For both \gls{FFS} and \gls{FFFS} with the \gls{UBC} enabled, the \SI{262\,144}{\kilo\byte} file size had significantly worse performance in the read test compared to the other file sizes of the same test for the filesystems. For \gls{FFFS} with the \gls{UBC} disabled, the read test for the same file size is similar to the performance of the preceding file sizes, but the performance is still significantly worse than for \gls{FFFS} with the \gls{UBC} enabled. It is therefore probable that \gls{FFS} with the \gls{UBC} disabled would perform similarly for the \SI{262\,144}{\kilo\byte} file size as it preforms for the preceding file sizes but still preform significantly worse than for the same file size for \gls{FFS} with the \gls{UBC} enabled.

For the \mbox{re-read} and random read tests, \gls{APFS} with the \gls{UBC} disabled performs similar to \gls{APFS} with the \gls{UBC} enabled, while \gls{APFS} with the \gls{UBC} is significantly worse than \gls{APFS} with the \gls{UBC} enabled for the read test. This indicates that the cache of \gls{APFS} is fast for previously read files. This is expected for a \mbox{widely-used} commercial filesystem.

Looking at Figure~\ref{fig:res_box_write}, Figure~\ref{fig:res_box_rewrite}, and Figure~\ref{fig:res_box_rndwrite}, it is observable that all three write tests of \gls{FFS} with the \gls{UBC} enabled have \mbox{high-performing} outlier data points. This can also be seen in Figure~\ref{fig:bench_ffs_with_cache} where the low-performance values have high bars and the different bars per file size are overlapping, but a few, shorter bars have much higher performance. These outliers are even outperforming the best performance data points of both \gls{GCSF} and \gls{FFFS} with both states of the \gls{UBC}. The reason behind these outliers is unclear. As \gls{FFS} with the \gls{UBC} disabled does not have similar outliers, and as these extreme values are not coming from the \SI{262\,144}{\kilo\byte} file size which the benchmarks of \gls{FFS} with the \gls{UBC} disabled does not include, it is possible the extreme values are due to the \gls{UBC}. Furthermore, \gls{GCSF} and \gls{APFS} with the \gls{UBC} enabled also have some outlier data points in the three figures similar to the style of the outliers of \gls{FFS} with the \gls{UBC} enabled. This further indicates that the outliers are due to the \gls{UBC}. Although, \gls{FFFS} does not have similar performance outliers for either state of the \gls{UBC}. This could indicate that the performance of the \gls{FFFS} writes are as high as possible and is not affected by potential performance increases by the \gls{UBC}.

\gls{FFS} with the \gls{UBC} enabled performed better than \gls{GCSF} with the \gls{UBC} enabled for all the read tests. However, \gls{FFS} with the \gls{UBC} disabled performs significantly worse than \gls{GCSF} with the \gls{UBC} disabled. This indicates that the \gls{UBC} is critical in the performance of \gls{FFS} while the performance of \gls{GCSF} depends less on the \gls{UBC}. This could be due to a faster cache implemented by \gls{GCSF} compared to the cache of \gls{FFS}. It could also mean that Google Drive is a more \mbox{time-efficient} storage service than Flickr as both filesystems might have to communicate more with the \gls{OWS}s when the \gls{UBC} is disabled. As Google Drive provides the raw data of the file stored, \gls{GCSF} can store the raw data in its cache meaning that the data in the read operation can be returned faster. If Google Drive caches the raw file data as well, it does not have to decrypt the data when serving it to \gls{GCSF}. Looking at the median performance of \gls{FFS} and \gls{GCSF}, \gls{GCSF} with both states of the \gls{UBC} outperforms \gls{FFS} with both states of the \gls{UBC}, in all the write tests. The reason could be that \gls{GCSF} does not have to encrypt the data nor encode it as images. This could save significant computation time. Furthermore, it is possible that \gls{GCSF} is faster because it requires fewer REST \gls{API} calls. As Google Drive is a filesystem, the inode table of the filesystem, or how ever the filesystem is organized, can be stored on the service without exposing it to the user. For instance, assuming that Google Drive uses an inode table like \gls{FFS}, the inode table would never have to be downloaded and uploaded by \gls{GCSF}. By simply uploading a file and specifying its path and filename, the inode table does not have to be modified by the user but can be handled by Google Drive in the background, potentially after the request has completed requiring less time for the file upload request. Meanwhile, \gls{FFS} has to upload the inode table after every file modification. Additionally, the old version of the file and inode table must be removed. This requires \gls{FFS} to perform at least four requests for all modifications:
\needspace{5\baselineskip}
\begin{itemize}
	\item Upload a new image with the new file content,
	\item Upload a new image with the new inode table content,
	\item Remove the old file, and,
	\item Remove the old inode table
\end{itemize}
Although, removing the old images is performed on another thread and does not affect the filesystem operation time. However, it can affect the congestion of the bandwidth to the \gls{OWS} if it is performed at the same time as another request, such as a file upload. Meanwhile, uploading a modified file to Google Drive requires one \gls{API} call using the file's ID\,\cite{FilesUpdateDrive2022} which will perform the same functionality as the four requests required for \gls{FFS}. The ID of the file could be stored locally in memory by \gls{GCSF} to be able to serve file ID's quickly, but this data structure does not have to be uploaded to Google Drive. Furthermore, when downloading a file, parts of the file can be downloaded rather than the full file\,\cite{googleDownloadFilesDrive2022}. This can reduce the time as the full file does not have to be downloaded every time a file is read. Even if we could download parts of a file from Flickr, it would not make sense for \gls{FFS} as we need the full file content to decode the image pixel data as the encrypted cipher text. Even if we could download certain blocks of the cipher text, we need the whole cipher text to verify the authenticity of the \mbox{\gls{AES}-\gls{GCM}} cipher. We could also not change the content of this block and encrypt it again without reusing the cryptographic parameters, thus, for write operations we need the full file. Furthermore, by downloading the full image when it is read, subsequent reads of \gls{FFS} are faster which is beneficial for the benchmarking tests as it often needs to read the file multiple times at different offsets. Google Drive has 800 million daily users\,\cite{lardinoisGoogleUpdatesDrive2017} versus Flickr's 60 million monthly users\,\cite{campbellFlickrStatistics20222022}, which means Google Drive is a much bigger service. This could mean that Google Drive has better infrastructure and that they can process the uploaded data faster and serve the data for download faster than Flickr can.

For the write tests on \gls{FFS} and \gls{GCSF} with the \gls{UBC} disabled, the file size is significant for the performance of the filesystems, as can be seen in Figure~\ref{fig:bench_ffs_without_cache} and Figure~\ref{fig:bench_gcsf_without_cache}. In general, a larger file sizes has better performance than smaller file sizes for these filesystems. The histograms in Figure~\ref{fig:bench_ffs_without_cache} and Figure~\ref{fig:bench_gcsf_without_cache}  show almost isolated peaks for each file size, while the histograms for the other tests of the filesystems show more overlap of the performances of the different file sizes. The distribution of these file sizes looks to be normally distributed. For \gls{FFFS} with the \gls{UBC} disabled, the bars in the histogram overlap more than the bars in \gls{FFS} and \gls{GCSF}, but the peaks of the different file sizes are still distinguishable (as shown in Figure~\ref{fig:bench_fffs_without_cache}). The distribution of these values also seem to be normally distributed. For \gls{APFS} with the \gls{UBC} disabled, the histogram does not show the same pattern as seen with \gls{FFS}, \gls{GCSF}, and \gls{FFFS}. The values of \gls{APFS} with the \gls{UBC} disabled does not seem to be normally distributed.

The only implementation difference between \gls{FFS} and \gls{FFFS} is that \gls{FFS} stores the produced images on Flickr while \gls{FFFS} stores the produced images on the local filesystem. Therefore, the time difference of an \gls{FFS} operation compared to an \gls{FFFS} operation should only depend on the internet connection to Flickr and how fast Flickr can process the requests, if we assume the \gls{UBC} is not enabled. For instance, looking at the write tests when the \gls{UBC} is disabled for the two filesystems, \gls{FFFS} outperforms \gls{FFS} significantly. Looking at one file size, for instance, \texttt{file size = 8\,192} \gls{FFFS} has an average performance of approximately \SI[per-mode = symbol]{7\,500}{\kilo\byte\per\second} and \gls{FFS} has an average performance of approximately \SI[per-mode = symbol]{870}{\kilo\byte\per\second} for the write test. This means that the test took on average \SI{1\,092}{\milli\second} for \gls{FFFS} and \SI{9\,416}{\milli\second} for \gls{FFS}. The same test for the same file size for \gls{APFS} with the \gls{UBC} enabled\footnote{Even though we are looking at \gls{FFS} and \gls{FFFS} with the \gls{UBC} disabled, filesystem calls to \gls{APFS} will not be affected, thus the default state of the \gls{UBC} will be used when \gls{FFS} and \gls{FFFS} interacts with \gls{APFS}} had an average performance of around \SI[per-mode = symbol]{534\,000}{\kilo\byte\per\second}, meaning it took on average \SI[per-mode = symbol]{15}{\milli\second} for \gls{APFS} to save the \SI[per-mode = symbol]{8\,192}{\kilo\byte} file. The time the test takes for \gls{FFFS} is the same as the write operation overhead plus the time \gls{APFS} takes to save the file. Subtracting the \gls{APFS} write time from the test time of \gls{FFFS}, we get the average overhead of the Write test for both \gls{FFS} and \gls{FFFS} as they have the same overhead. This overhead includes encrypting the data and encoding the result as an image. The average overhead time of the Write test for \gls{FFS} and \gls{FFFS} is therefore \SI[per-mode = symbol]{1\,077}{\milli\second}, meaning that the request and the request's overhead by \gls{FFS} took on average $9\,416 - 1\,077 =$ \SI{8\,339}{\milli\second} which is approximately 88\% of the computation time for this test.  % TODO: WHY MAXIMUM?? AVERAGE IS OBV TOO LOW BUT ARGUEE!!
Assuming the upload bandwidth to Flickr while \gls{FFS} with the \gls{UBC} disabled was being benchmarked is the same as the measured maximum bandwidth of the computer, i.e. \SI[per-mode = symbol]{17\,720}{\kilo\bit\per\second}, uploading \SI[per-mode = symbol]{8\,192}{\kilo\byte} = \SI[per-mode = symbol]{16\,536}{\kilo\byte} would take \SI[per-mode = symbol]{933}{\milli\second}. This means that the remaining \SI[per-mode = symbol]{7\,406}{\milli\second} were used for request overhead, such as preparing for the request, waiting for Flickr to process the data, and receiving the response from Flickr, including the post ID. Preparing the request includes first saving the image to the local filesystem, and then reading it when uploading it. Assuming the file was saved with the same \gls{APFS} performance as above, it would take \SI[per-mode = symbol]{27}{\milli\second} to save the file. Reading the file with the average read test performance of \gls{APFS} with the \gls{UBC} enabled for a \SI[per-mode = symbol]{8\,192}{\kilo\byte} file of \SI[per-mode = symbol]{6\,906\,000}{\kilo\byte\per\second} would take \SI[per-mode = symbol]{1}{\milli\second}. The remaining \SI[per-mode = symbol]{7\,378}{\milli\second} are used for creating the request, receiving the request response, and for Flickr to process the data. The most significant time consumption of these three tasks is most probably the data processing of Flickr. This indicates that with faster data processing by Flickr, \gls{FFS} could potentially be faster. Furthermore, it indicates that the bandwidth of the internet connection to the \gls{OWS} is not the most important factor of the performance of \gls{FFS}. Even if uploading the file over the internet to the \gls{OWS} would be instant, it would only reduce the file operation time by around 10\%. To improve the filesystem operation performance, using a \gls{OWS} that can process the data faster is of more importance. For instance, if the \gls{OWS} would process the data in the background and instantly return, the write operation performance could be reduced. This could mean, however, that the image is not seen on the \gls{OWS} instantly which could mean that it is not possible to download the file instantly after it has been uploaded.  %TODO: NOT TRUE; CHANGE TO NEW BANDWIDTH STUFF
The calculations above assume that the bandwidth to Flickr was approximately the same as the bandwidth to the measurement servers. It is possible that the bandwidth to Flickr was much lower, which would mean that the bandwidth has more impact of the filesystem operation time. Future work includes using multiple \gls{OWS}s to compare their performances, as well as measuring the individual filesystem's bandwidth for more precise calculations.

The methodology to collect bandwidth data of the filesystems has to improve for future work. In this these, the bandwidth was measured by measuring the overall bandwidth of the computer, both incoming and outgoing packets, of all processes running on the computer sending or receiving http requests. The packets were captured over \mbox{ten-second} intervals. As it was not possible to ensure that no other process using the internet was running on the computer during the benchmarking, this measurement includes the bandwidth used by other processes. Furthermore, as there is no difference in the bandwidth result between upload bandwidth and download bandwidth, it is not possible to analyze the upload bandwidth and download bandwidth individually. Measuring the bandwidth of \gls{FFS} can be done by adding measurements inside the code that, for each request to Flickr, creates a log message of the size and duration of each request. Furthermore, creating log messages for write and read operations from \gls{FUSE} including information such as the buffer size and offset could possibly allow us to associate the network requests with the corresponding test case of IOZone. This could possibly allow us to analyze the bandwidth per file operation, per file size, and per buffer size.

Analyzing the bandwidth of other \mbox{cloud-based} filesystems, such as \gls{GCSF}, as rigorously as can be done for \gls{FFS} is probably not possible without modifying the source code of the other filesystem. However, better measurements than the ones performed by this thesis could be done. For instance, using wireshark by filtering based on the source and destination IP address. If each packet sent from and to the computer is captured, these can be analyzed to look at DNS lookups to the \gls{OWS}, for instance, \texttt{www.googleapis.com}. All packets sent to or originating from the IP addresses from the DNS lookups can be filtered. By separating the packets sent from the computer from the packets sent to the computer, the upload and download bandwidth can be analyzed separately. It is possible that other processes are sending requests to the same APIs which would mean that these packets also would be captured, but there would probably be less packets captured than capturing all packets sent and received by the computer. 

%
% Low bandwidth for FFS without UBC because it is executed very slowly
%	Average bandwidth gets lower
%
% Talk about low bandwidth - idle/local computations or waiting for responses
%
%
%
%
%
%
%

% Figure~\ref{fig:bench_ffs_read}, Figure~\ref{fig:bench_ffs_re_read}, and Figure~\ref{fig:bench_ffs_rnd_read} show that \gls{FFS} performs poorly for the Read tests with a small buffer size. Beginning at a \SI{4}{\kilo\byte} buffer size, the performance in general increases with the first few buffer sizes. This could be influenced by the overhead of the \gls{FFS} read operation. Overhead of the read operation includes, among other things, the time to fetch the images from Flickr if they are not in the filesystem cache, and decrypting the images which is required even if the images are in the filesystem cache. Further, it is expected that the \mbox{Re-Read} tests perform better than the Read tests when the file size is small enough to fit in the filesystem cache. This is also a conclusion that can be drawn from the result. However, looking at Figure~\ref{fig:bench_ffs_read}, and Figure~\ref{fig:bench_ffs_re_read}, we can see that the \mbox{Re-Read} tests overall performs better for file sizes bigger than the \SI{5}{\mega\byte} cache file size limit as well. Especially for file sizes \SI{32\,768}{\kilo\byte} to \SI{262\,144}{\kilo\byte}, the performance of the Read tests is in general very low compared to the \mbox{Re-Read} tests. As Figure~\ref{fig:res_box_read} and Figure~\ref{fig:res_box_reread} shows, the average performance of the \mbox{Re-Read} tests are more than double the average performance of the Read tests for \gls{FFS}. The reason why the \mbox{Re-Read} tests perform better than the Read test for files bigger than the filesystem cache size limit is most probably due to kernel caching of the files. The performance is higher than a usual internet connection bandwidth would be. An internet connection's bandwidth is often limited to a maximum of $\text{\SI[per-mode = symbol]{1}{\giga\bit\per\second}} = \text{\SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}}$ by the ISP, depending on the subscription. For the read operations of the \mbox{cloud-based} filesystems that exceed \SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}, the data has most probably been provided my some kind of cache. Note that this limit is much higher than the measured reference bandwidth presented in Section~\ref{sec:res_bench}, but we use this limit as a reference of a maximum bandwidth of the connection rather than as the actual bandwidth of the connection. All the data points for the \mbox{Re-Read} tests for \gls{FFS} and \gls{GCSF} exceed \SI[per-mode = symbol]{1\,000\,000}{\kilo\byte\per\second} indicating that all these files were served by some cache. For \gls{FFS}, the files over \SI[per-mode = symbol]{5}{\mega\byte} were most probably served from the kernel cache as they cannot fit in the filesystem cache. As the performances differ significantly between the filesystems, it is possible that not all data was provided by the kernel cache, but that some data was provided by internal filesystem caches. Otherwise, the data could be more similar, assuming the kernel cache has the same performance for all filesystems tested. Many data points of the Read tests for \gls{FFS} and \gls{GCSF} also exceed \SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}, indicating that some data returned when running this test were also served by some cache. This data could be saved in the cache after the \mbox{Re-Write} tests which precedes the Read tests. The data could also be cached after the first Read test of the file which could explain the increasing performance for the smaller buffer sizes in the beginning of the tests. The biggest file sizes have lower performance than the other file sizes for the Read tests of \gls{FFS} and \gls{FFFS}, although the performance is still higher than the maximum bandwidth reference point of \SI[per-mode = symbol]{125\,000}{\kilo\byte\per\second}, indicating that the kernel cache has worse performance for bigger files.

% % TODO: No, IOZone closes the file. Remove. Can we keep some discussion?
% Other than the data being provided by the kernel cache, it is also possible that IOZone does not close the file before it is read again, meaning that the file can be kept in memory for \gls{FFS} and \gls{FFFS} as they cache open files that have been read. IOZone does not specify when the file is closed, however it could be assumed that the files are closed between tests. If the file would not be closed after a write test, the write tests of \gls{FFS} should have similar performance for the write tests as the write tests of \gls{FFFS} as neither of the two filesystems save the data in their storage medium until the file has been closed. As the write tests of \gls{FFS} and \gls{FFFS} are not similar it is improbable that the file is not closed after the write tests. The \mbox{Re-Read} test is performed after the Read test, and as the \mbox{Re-Read} test is significantly faster than the Read test for both \mbox{FFS} and \gls{FFFS}, even for files bigger than the cache limit of the filesystems, it is possible that the files are not closed between the two tests. However, if this was the case, the \mbox{Re-Read} tests would probably be more similar for \gls{FFS} and \gls{FFFS} than the results show, as the data could be served just as fast by both filesystems. The theory that the kernel cache is providing a better performance of the \mbox{Re-Read} is more probable as the tests are less consistent between the two filesystems. If the kernel would not have the requested data in its cache, the filesystem would have to be called. This could be the explanation why the filesystem performances differ significantly between the two filesystems. Cache misses in the kernel cache requires \gls{FFS} and \gls{FFFS} to get the data from their cache if it is present there, or from their respective storage medium. This introduces performance differences between the filesystems. Cache misses in the filesystem cache is less probably as there are no more files being saved, meaning that the files will not be removed from the cache.

% While the \mbox{Re-Read} and Random Read tests increases in performance for the first buffer sizes for \gls{FFS}, the performance also decreases eventually. Looking at the data presented in Figure~\ref{fig:bench_ffs_re_read} and Figure~\ref{fig:bench_ffs_rnd_read}, the buffer sizes \SI[per-mode = symbol]{4\,096}{\kilo\byte} to \SI[per-mode = symbol]{16\,384}{\kilo\byte} have, in general, lower performance than the buffer sizes \SI[per-mode = symbol]{256}{\kilo\byte} to \SI[per-mode = symbol]{2\,048}{\kilo\byte}, for the same file size. This indicates that the optimal buffer size for the \gls{FFS} read operations on previously read files is not the biggest possible buffer size, but rather around \SI[per-mode = symbol]{512}{\kilo\byte}, depending on the file size. The biggest file size has its best performance for a buffer size of \SI[per-mode = symbol]{512}{\kilo\byte}. Looking at the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size, it peaks for both the \mbox{Re-Read} test and the Random read test at \texttt{buffer size = 128\,kB}, and its performance for that buffer size for both tests are higher than any other performance of any file size or buffer size in both tests, for the filesystem. This is interesting because the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size does not always outperform the other file sizes. Looking at the \mbox{Re-Read} test, the \SI[per-mode = symbol]{262\,144}{\kilo\byte} has the best performance for seven of the 13 tests while the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size has the best performance for five buffer sizes, namely the first one, the last three and for \texttt{buffer size = 128\,kB}. The \SI[per-mode = symbol]{16\,384}{\kilo\byte} file size has the best performance of the test once, for \texttt{buffer size = 1\,024\,kB}. Considering how fast the operations actually are, it is possible to understand why the values can fluctuate. For instance, the \mbox{Re-Read} test for \gls{FFS} using \texttt{file size = 16\,384\,kB, buffer size = 1\,024\,kB} has a performance of \SI[per-mode = symbol]{7\,599\,353}{\kilo\byte\per\second}. Transferring \SI[per-mode = symbol]{16\,284}{\kilo\byte} at \SI[per-mode = symbol]{7\,599\,353}{\kilo\byte\per\second} takes \SI[per-mode = symbol]{2.14}{\milli\second}. If it would take 10\% more time, the performance would instead be under \SI[per-mode = symbol]{7\,000\,000}{\kilo\byte\per\second}, meaning that the \SI[per-mode = symbol]{262\,144}{\kilo\byte} file size would have better performance for the same buffer size. However, if it instead would take 10\% less time to perform the \texttt{file size = 16\,384\,kB, buffer size = 1\,024\,kB} \mbox{Re-Read} test, it would reach a performance of \SI[per-mode = symbol]{8\,443\,726}{\kilo\byte\per\second} which would be the highest performance of the test on \gls{FFS} of all file sizes and buffer sizes. Small time differences can significantly affect the performance of the tests. The time of the filesystem operations can be fluctuated by process scheduling and the performance of the kernel cache, among other things.

% The performance of the write operations of \gls{FFS} are highly influenced by the file size. As shown in Figure~\ref{fig:bench_ffs_write}, Figure~\ref{fig:bench_ffs_re_write}, and Figure~\ref{fig:bench_ffs_rnd_write}, bigger file sizes implicate better performance for the write operations, generally. The best performing file size was most often the largest file size, \SI[per-mode = symbol]{262\,144}{\kilo\byte}. Furthermore, the biggest file size of the tests perform better for the Write test than for the \mbox{Re-Write} and Random Write test. This is interesting as the Write test includes the overhead of creating the files before writing to them, which \mbox{Re-Write} and Random Write does not.

% Certain data points in the graphs presented in Section~\ref{sec:res_bench} are outliers from groups of data points. For instance, looking at the Random Write test for \gls{GCSF} in Figure~\ref{fig:bench_gcsf_re_read} for \texttt{file size = 65\,536\,kB, buffer size = 16\,384\,kB}, the test data point has significantly lower performance than the other data points for the other buffer sizes in the same test with the same file size. There are many possible reasons behind this drop in performance. One reason could be a slow internet connection to Google Drive or slower data processing by Google Drive, for instance, due to a higher load of other user requests to the service at the point in time when the specific data point was benchmarked. Due to the \gls{OWS} being an external service that other users use at the same time, it is always possible that the \gls{OWS} of the \mbox{cloud-based} filesystems experience a high user demand at any time. Another reason for the data outlier can also be because the operating system scheduler scheduled the \gls{GCSF} process unfavorable at that time. This is an especially possible reason for the outlier data points for the \mbox{non-cloud-based} filesystems \gls{FFFS} and \gls{APFS} as they do not rely on an internet connection or an \gls{OWS}. For instance, Figure~\ref{fig:bench_apfs_rnd_read} shows two outliers for \gls{APFS} for \texttt{file size = 131\,072\,kB}, namely \texttt{buffer size = 4\,096\,kB} and \texttt{buffer size = 8\,192\,kB}. They could also have lower performance due to disk scheduling and cache management. The files in the \gls{APFS} cache could be invalidated and removed if other processes are reading other files from the disk at the same time, resulting in a cache miss of the benchmark file.

% Other outlier data points have much higher performance than the other data points in a test. For instance, looking at the Write test for \gls{FFFS} in Figure~\ref{fig:bench_fffs_write}, there are data points for \texttt{file size = 8\,192\,kB}, \texttt{file size = 16\,384\,kB}, and \texttt{file size = 32\,768\,kB} that have much higher performance than the other data points. While most data points are approximately between \SI[per-mode = symbol]{6\,500}{\kilo\byte\per\second} to \SI[per-mode = symbol]{8\,300}{\kilo\byte\per\second}, two of these file sizes have one outlier, and one has two outliers, of approximately \SI[per-mode = symbol]{100\,000}{\kilo\byte\per\second}. Outliers can also be seen in Figure~\ref{fig:bench_fffs_re_write} and Figure~\ref{fig:bench_fffs_rnd_write} for the \mbox{Re-Write} and Random Write tests on \gls{FFFS}. \gls{FFFS} is not a cloud-based filesystem and is therefore not affected by, for instance, a fluctuating bandwidth of an internet connection. Rather, this is possibly the result of favorable process scheduling and disk operation scheduling. As both file sizes exceed the cache limit of \gls{FFFS}, the cache of the filesystem should not affect the value. However, it is also a possibility that IOZone did not close the file after the test or that the \texttt{close} operation was not performed correctly after test, which would mean that the data would not be written to the storage medium resulting in shorter execution time, leading to a higher performance for the write tests. The data of the open file would also be stored in the memory of \gls{FFFS}. Therefore, if there was a missed \texttt{close} operation, a subsequent read test should also be an outlier data point with higher performance than the other data points. Considering the outlier for the \mbox{Re-Write} test for \texttt{file size = 16\,384\,kB, buffer size = 8\,192\,kB}, and looking at the subsequent Read test for \texttt{file size = 16\,384\,kB, buffer size = 8\,192\,kB}, the data point for the Read is not an outlier. This indicates that the outlier in the \mbox{Re-Write} test probably is not due to a missed \texttt{close} call. Rather, it is possible that the higher performing \mbox{Re-Write} tests outliers are due to process scheduling. 

% Comparing \gls{FFFS} benchmarking results against the \gls{APFS} benchmarking results, we can compare the theoretical best performance of \gls{FFS} against a \mbox{general-purpose} \mbox{widely-used} filesystem. Furthermore, we can compare \gls{FFFS} against the underlying filesystem in which it is storing its data. In Figure~\ref{fig:bench_fffs_read} and Figure~\ref{fig:bench_apfs_read} we can see that the read operation perform similarly for \gls{FFFS} and \gls{APFS}, where \gls{APFS} is in general faster than \gls{FFFS}. However, for certain data points, such as \texttt{file size = 131\,072\,kB, buffer size = 256\,kB}, \gls{FFFS} has a higher performance than \gls{APFS} with \SI[per-mode = symbol]{7\,525\,973}{\kilo\byte\per\second} for \gls{FFFS} and \SI[per-mode = symbol]{5\,927\,107}{\kilo\byte\per\second} for \gls{APFS}. As the file size is bigger than \SI{5}{\mega\byte} it was not stored in the cache of \gls{FFFS}. The reason why \gls{FFFS} outperformed \gls{APFS} at this data point is could be due to process scheduling or because the buffer size used by the test is less efficient for \gls{APFS} than the one called on \gls{APFS} by \gls{FFFS}. The buffer size used by \gls{FFFS} to read the file from \gls{APFS} is not certain to be the same as \gls{FFFS} was called with. The buffer size used by \gls{FFFS} on \gls{APFS} is set by the implementation of \texttt{basic\_filebuf}\,\cite{cppreference.comStdBasicFilebuf2020}. This has been found through testing to be the same size as the image written by \gls{FFFS} to \gls{APFS} if it is less or equal than \SI[per-mode = symbol]{65\,536}{\byte}, otherwise multiple buffers are used where the biggest ones are \SI[per-mode = symbol]{65\,536}{\byte}. It is also possible that the data was provided by the kernel cache to \gls{FFFS} while the same test for \gls{APFS} did not get the data from the kernel cache. Another possibility is that \gls{FFFS} read the data from \gls{APFS}, but it was provided by a cache of \gls{APFS} rather than being read from the \gls{APFS} storage device, while the same test for \gls{APFS} read the data from the storage device.

% The cache of a filesystem can generally influence the performance of the read operations. However, there is no significant drop in performance for \gls{FFFS} when reading a file that fits in the \gls{FFFS} cache, and one that does not. In fact, the performance of the Read test for \texttt{file size = 8\,192} is in general better than for \texttt{file size = 4\,096} and \texttt{file size = 2\,048} on \gls{FFFS}, even though the \SI{8\,192}{\kilo\byte} file cannot fit in the \gls{FFFS} cache while the other file sizes can, as long as the encrypted data and the PNG attributes do not make the file bigger than \SI[per-mode = symbol]{5}{\mega\byte}. The reason why these files can be provided fast is most probably due to the kernel caching of the files. However, the performance of the \texttt{file size = 262\,144\,kB} compared to many of the other file sizes is much lower, indicating that these files were not, at least entirely, cached by the kernel, possibly due to size constraint. As the \SI[per-mode = symbol]{131\,072}{\kilo\byte} file size has similar performance to the \SI[per-mode = symbol]{262\,144}{\kilo\byte} file size for some of the \gls{FFFS} Read tests buffer sizes, but significantly higher performance for other buffer sizes, it is possible that the kernel cache size limit is around \SI[per-mode = symbol]{131\,072}{\kilo\byte}. However, as it seems to fluctuate, the size limit might be dependent on factors such as the available memory of the system at the time. When the memory usage is high, for instance, by other processes running at the same time, it is possible that the data stored in the kernel cache for \gls{FFFS} is removed, which decreases the performance for certain tests. 

% While the values of the read operation for \gls{FFFS} and \gls{APFS} are comparable to each other, this is not the case for all tests. For instance, the write operation of \gls{FFFS} is much slower than the write operation of \gls{APFS}. The write operation performance average of \gls{FFFS} is approximately 1.5\% of the average performance of the write operation on \gls{APFS}. While it is understandable that the \gls{FFFS} write performance is lower than the \gls{APFS} write performance as all data written to \gls{FFFS} must also be written to \gls{APFS}, the difference in performance is significant. The reason for this could be the fact that \gls{FFFS} has to encrypt the data stored, including creating all the cryptographic variables such as the salt and the \gls{IV}. Using the calculations above, we estimate that the average overhead of an \gls{FFFS} write operation for a \SI[per-mode = symbol]{8\,192}{\kilo\byte} file is \SI[per-mode = symbol]{505}{\milli\second} out of the total \SI{518}{\milli\second} for the filesystem operation, which is 97\% of the time of the operation. Assuming that this number is similar for other file sizes, it is a very significant portion of the execution time. While \gls{APFS} is also an encrypted filesystem, it is possible that the filesystem prepares the cryptographic variables before they are needed. For instance, the next cryptographic key and its salt could be derived while the filesystem is idle as it does not depend on the content of the stored data. It is also possible that \gls{APFS} is using multiple threads to write the data which could speed up the operation. For instance, it is possible that \gls{FFFS} requires multiple \texttt{write} calls to the underlying \gls{APFS} filesystem. If each \texttt{write} call to \gls{APFS} is computed on a different thread, the multiple \texttt{write} calls could possibly be completed faster rather than if they were completed sequentially.

% \gls{FFFS} and \gls{GCSF} are comparable in some tests, which is interesting as \gls{GCSF} is dependent on an internet connection and an \gls{OWS} while \gls{FFFS} is not. The median performance of the \mbox{Re-Read} test on \gls{GCSF} is slightly worse than the medium performance of the \mbox{Re-Read} test on \gls{FFFS}. Meanwhile, the median Read performance of \gls{GCSF} is significantly less than the median Read performance of \gls{FFFS}. This indicates that a lot of the data served by \gls{FFFS} and \gls{GCSF} was served by the kernel cache of the \gls{FUSE} filesystems. Furthermore, it indicates that \gls{FFFS} is faster than \gls{GCSF} for files not in the kernel cache. However, as the Read tests performance is higher than a reasonable internet bandwidth, it is probable that the data from \gls{GCSF} is still served from an internal cache rather than the data being fetched from Google Drive. As the Read tests performances are higher for \gls{FFFS} than for \gls{GCSF}, it indicates that \gls{FFFS} has a faster internal cache than \gls{GCSF}, or that the kernel cache was used for the \gls{FFFS} Read tests while it was not used for \gls{GCSF}.

% The Write, \mbox{Re-Write}, and Random Write tests on \gls{FFFS} outperform the same tests on \gls{GCSF}. This is reasonable as the data written to \gls{GCSF} must be uploaded to Google Drive, while the data written to \gls{FFFS} is stored on the local disk. Uploading \SI{16}{\mega\byte} of data with the average (reference point) upload speed of \SI[per-mode = symbol]{91.83}{\mega\bit\per\second} would take approximately \SI{1.4}{\second}. Meanwhile, we can see in Figure~\ref{fig:bench_apfs_write} that \gls{APFS} can write \SI{16}{\mega\byte} of data as fast as \SI[per-mode = symbol]{6\,921\,222}{\kilo\byte\per\second} = \SI[per-mode = symbol]{6\,921.2}{\mega\byte\per\second}, meaning it would take approximately \SI{2}{\milli\second} to write the data. Meanwhile, the maximum Write performance of \gls{FFFS} is \SI[per-mode = symbol]{101\,677}{\kilo\byte\per\second}, meaning that \gls{FFFS} can write \SI{16}{\mega\byte} in approximately \SI{157}{\milli\second}. With this data, we can see that \gls{FFFS} can write the \SI{16}{\mega\byte} of data approximately 7\,800\% slower than \gls{APFS} can, and that \gls{GCSF} can write the \SI{16}{\mega\byte} of data approximately 800\% slower than \gls{FFFS} or approximately 70\,000\% slower than \gls{APFS}. 

% It is easy to see, and it is not unexpected, that \gls{APFS} outperforms \gls{FFS} in performance. As a professional local filesystem, \gls{APFS} will always have better performance than FFS. Further, like \gls{FFFS}, the performance of \gls{FFS} depends on the performance of \gls{APFS} as the file which is uploaded to Flickr first needs to be saved on disk. This dependency could be removed, for instance, by providing the temporary file to the FlickCURL library via a \gls{FUSE} filesystem. Further, the median performance of the \mbox{Re-Read} test on \gls{FFS} is approximately 72\% of the performance of the \mbox{Re-Read} test on \gls{APFS}. This is probably high for many of the tests because of the kernel cache of \gls{FFS}, but as it differs significantly from \gls{APFS}, \gls{FFS} calls are probably also performed. With higher bandwidth and with another \gls{OWS} with faster data processing, it is possible that \gls{FFS} could increase its read performance. In contrast, the median performance of the \mbox{Re-Read} test on \gls{FFS} is approximately 76\% of the median performance of the same test on \gls{GCSF}.

% Generally, the figures of the benchmark data presented in Section~\ref{sec:res_bench} have visibly similar patterns for the Write tests and similar patterns the Read tests, per filesystem. For instance, the patterns of the \gls{FFS} Read, \mbox{Re-Read}, and Random Read tests follow a similar pattern with a similar curve of the data points, while the Write, \mbox{Re-Write}, and Random Write tests follow another distinct pattern. The Read, \mbox{Re-Read}, and Random Read figures of \gls{GCSF} follow another distinct pattern, as well as the three Write test data follow a fourth distinct pattern. The three write test patterns of \gls{FFS} are dissimilar to the patterns of the \gls{FFFS} write tests even though both filesystems are implemented very similarly, other than the storage medium. However, some patters are similar even though they are from different filesystem, for instance, the Read tests of \gls{FFS} and \gls{FFFS}. In Figure~\ref{fig:bench_ffs_read} and Figure~\ref{fig:bench_fffs_read}, the Read test shows that certain file size data points are found on the lower spectrum of the plot, while the other file sizes follow a somewhat similar curve for both filesystems. However, the pattern of the \mbox{Re-Read} test data of \gls{FFS} and \gls{FFFS} differ significantly. It might be possible to use these distinct patterns to create a fingerprint of a filesystem. This could be used, for instance, to identify a filesystem based on its performance when the filesystem is unknown. This could be useful when the filesystem is masked by an overlaying filesystem such as Cryptfs. However, by benchmarking the overlying filesystem, the pattern of the underlying filesystem might be lost. Analyzing if it is possible to identify filesystems based on benchmark fingerprints and identifying underlying filesystems using this technique is part of future work.

% The bandwidth reference measurements are performed every five minutes while \gls{FFS} and \gls{GCSF} are benchmarked. Each bandwidth measurement test takes about \SI[per-mode = symbol]{20}{\second} each. During this time, the measurement tool is uploading and downloading data to the measurement servers. This can cause congestion of the internet connections of the filesystems which could reduce their performance. Bredbandskollen estimates that each measurement requires approximately as half as many bytes as the measurement shows in bits per second\,\cite{internetstiftelsenMerOmBredbandskollen}. The average bandwidth measurements while the two filesystems were running were both approximately \SI[per-mode = symbol]{90}{\mega\bit\per\second} for both the upload and download speeds. This means that the measurements consumed about \SI[per-mode = symbol]{45}{\mega\byte} per measurement. This is bigger than most file sizes tested when benchmarking. Furthermore, background processes running at the same time as the filesystem benchmarks  and using internet connections can further congest the internet connection of the filesystems. If we could measure each filesystem's individual internet connection's bandwidth, calculations regard the impact of the bandwidth could be more precise. This is part of future work.



% TODO: 
%
% - Compare difference between kernel cache performance
%	- all over 125 000 is kernel cache for gcsf and ffs read operations
% 	- how much does it fluctuate?
%	- create figure, scatter all points
%
% - Not super fair to use kernel cache/writing and asking for data directly
% 	- more normal use case could be writing data once in a while and asking for it once in a while
%	- Write tests are more fair though
%
%	- Critically analyze method of collecting data
%		- Only running once, using kernel RAM etc.
%
%
%
% - Compare FFS read with UBC disabled, for 200mb file - basically same value which is expected. Show in graph?