\section{Filesystems}
\label{sec:dis_fs}
Figure~\ref{fig:bench_ffs_read}, Figure~\ref{fig:bench_ffs_re_read}, and Figure~\ref{fig:bench_ffs_rnd_read} show that \gls{FFS} performs poorly for Read operations with a small buffer size. Beginning at \SI{4}{\kilo\byte} buffer size, the performance in general increases with the first few buffer sizes. This indicates that the overhead of the \gls{FFS} read operation is high as the performance gets better when it reads fewer buffers. Overhead of the read operation includes, among other things, the time to fetch the image from Flickr if it is not in the cache, and decrypting the image which is required even if the image is cached. Further, it is expected that \mbox{Re-Read} should perform better than Read when the file size is small enough to fit in the cache. This is also a conclusion that can be drawn from the result. However, looking at Figure~\ref{fig:bench_ffs_read}, \ref{fig:bench_ffs_re_read}, we can also see that \mbox{Re-Read} overall performs better for file sizes bigger than the \SI{5}{\mega\byte} cache file size limit as well. Especially for the file sizes \SI{32768}{\kilo\byte} to \SI{262144}{\kilo\byte}, the performance of the Read test is in general very low. As Figure~\ref{fig:res_box_read} and Figure~\ref{fig:res_box_reread} shows, the average performance of \mbox{Re-Read} is more than double than the average performance of Read for \gls{FFS}. It is expected that the cache will increase the performance of the filesystem. The reason that \mbox{Re-Read} performs better than the Read test for files bigger than the cache size limit could be due to a cache in Flickr. It is possible that Flickr prepares often-requested images in a cache that serves the image faster than less-requested images. This could influence the time required to get the image from Flickr. 

One interesting comparison is between the benchmark results of \gls{FFS} and \gls{GCSF}. Both filesystems are \mbox{cloud-based} \gls{FUSE} filesystems dependent on an internet connection to their respective storage servers. Looking at the box plots in Section~\ref{sec:res_bench}, we can see that \gls{GCSF} outperforms \gls{FFS} in both average performance and median performance for all tests. However, \gls{GCSF} does not have any data for the biggest file size while \gls{FFS} has data for it. Looking at Figure~\ref{fig:bench_ffs_re_read} and Figure~\ref{fig:bench_gcsf_re_read}, we can see that \gls{FFS} performs better than \gls{GCSF} for the bigger file sizes. It is possible that \gls{FFS} would perform better than \gls{GCSF} for the \SI{262144}{\kilo\byte} file size test if \gls{GCSF} could run the test. However, it is also possible that \gls{GCSF} would still perform better overall. One reason that \gls{GCSF} generally outperforms \gls{FFS} could be because the \gls{FFS} cache stores the encrypted version of the image, meaning that before the data is read, the image must first be decrypted and decoded. As Google Drive provides the raw data of the file stored, \gls{GCSF} can store the raw data in its cache meaning that the data in the read operation can be returned faster. \gls{GCSF} also outperforms \gls{FFS} in all the write tests. The reason could be that \gls{GCSF} does not have to encrypt the data nor encode it as images. This could save significant computation time. The average (reference point) bandwidth measured when the two filesystem benchmarks were run are similar, indicating that there was not a big difference in the internet connection to the measurement servers. However, as this does not measure the actual internet connection to the \gls{OWS}, the actual bandwidth of the filesystem could be different from this value. However, even assuming that the bandwidth of the internet connections of \gls{GCSF} and \gls{FFS} are equal, \gls{GCSF} can still benefit from fewer REST \gls{API} calls. As Google Drive is a filesystem, the inode table of the filesystem, or however the filesystem is organized, can be stored on the service without exposing it to the user. For instance, assuming that Google Drive uses an inode table like \gls{FFS}, the inode table would never have to be downloaded and uploaded by \gls{GCSF}. By simply uploading a file and specifying what its path and filename is, the inode table does not have to be modified by the user but can be handled by Google Drive in the background. Meanwhile, \gls{FFS} has to upload the inode table after every file modification. Additionally, the old version of the file and inode table must be removed. This requires \gls{FFS} to perform at least four requests for all modifications:
\begin{itemize}
	\item Upload a new image with the new file content,
	\item Upload a new image with the new inode table content,
	\item Remove the old file, and,
	\item Remove the old inode table
\end{itemize}
Meanwhile, uploading a modified file to Google Drive requires one \gls{API} request using the file's ID\,\cite{FilesUpdateDrive2022} which will perform the same functionality as the four requests required for \gls{FFS}. The ID of the file might be stored locally in memory by \gls{GCSF} to be able to serve file ID's quickly, but this data structure does not have to be uploaded to Google Drive. Furthermore, when downloading a file, parts of the file can be downloaded rather than the full file\,\cite{googleDownloadFilesDrive2022}. This can reduce the time as the full file does not have to be downloaded every time a file is read. Even if we could download parts of a file from Flickr for \gls{FFS}, it would not make sense as we need the full file content to decrypt it. Futhermore, with Google Drive's 800 million daily users\,\cite{lardinoisGoogleUpdatesDrive2017} versus Flickr's 60 million monthly users\,\cite{campbellFlickrStatistics20222022}, Google Drive is a much bigger service. This could mean that it has better infrastructure which can process uploaded data faster than Flickr can.

Certain data points in the graphs presented in Section~\ref{sec:res_bench} are outliers from groups of data points. For instance, looking at the \mbox{Re-Read} test for \gls{GCSF} in Figure~\ref{fig:bench_gcsf_re_read} for \texttt{file size = 32768kB, buffer size = 256kB}, the test data point has much lower performance than the the other data points for similar buffer sizes in the same test with the same file size. There are many possible reasons behind this drop in performance. One reason could be a slow internet connection to Google Drive in the point of time when the specific data point was benchmarked, for instance due to a higher load of other user requests to the service. Due to the \gls{OWS} being an external service that other users use at the same time, it is always possible that the \gls{OWS} of the \mbox{cloud-based} filesystems experiences a high \mbox{user-demand} at any time. Another reason of the data outlier can also be because the operating system scheduler scheduled the \gls{GCSF} process unfavorable at that time. This is especially possible reason to the outlier data points for the \mbox{non-cloud-based} filesystems \gls{FFFS} and \gls{APFS}. For instance, Figure~\ref{fig:bench_apfs_rnd_read} shows two outliers for \texttt{file size = 131072kB}, namely \texttt{buffer size = 4096} and \texttt{buffer size = 8192}. They could also have lower performance due to disk scheduling and cache management. The cached files could be removed from the cache if other processes are reading other files from the disk at the same time.

Other outlier data points have much higher performance than the other data points in a test. For instance, looking at the Write test for \gls{FFFS} in Figure~\ref{fig:bench_fffs_write}, there is one data point in both \texttt{file size = 16384} and \texttt{file size = 32768} that has much higher performance than the other data points. For some reason, while most data points are around \SI[per-mode = symbol]{6\,500}{\kilo\byte\per\second} to \SI[per-mode = symbol]{8\,300}{\kilo\byte\per\second}, both tests have one outlier of almost \SI[per-mode = symbol]{102\,000}{\kilo\byte\per\second}. \gls{FFFS} is not a cloud-based filesystem and is therefore not affected by a fluctuating internet connection. Rather, this is possibly the result of favorable process scheduling and disk operation scheduling. As both file sizes exceed the cache limit of \gls{FFFS}, the cache of the filesystem should not affect the value. However, it is a possibility that IOZone did not close the file or that the \texttt{close} operation was not performed correctly for the preceding test, which would keep the open file in memory regardless of size. This would result in faster file operation as the file would not have to be read from the disk. Furthermore, if the \texttt{close} operation was not called for a test, the filesystem would not write the data to the disk resulting in shorter execution time, leading to a higher performance. Therefore, if there was a missed \texttt{close} operation, the following test or preceding test should also have higher performance than the other data points. However, looking at, for instance, the preceding Read test for \texttt{file size = 32768, buffer size = 2048} in Figure~\ref{fig:bench_fffs_read} and the subsequent \mbox{Re-Read} test for \texttt{file size = 32768, buffer size = 2048} in Figure~\ref{fig:bench_fffs_re_read}, those data points are not outliers which indicates that it the outlier in the Write test probably is not due to a missed \texttt{close} call.

Comparing \gls{FFFS} benchmarking results against the \gls{APFS} benchmarking results, we can compare the theoretical best performance of \gls{FFS} against a \mbox{general-purpose} \mbox{widely-used} filesystem. Furthermore, we can compare \gls{FFFS} against the underlying filesystem in which it is storing its data. In Figure~\ref{fig:bench_fffs_read} and Figure~\ref{fig:bench_apfs_read} we can see that the read operation perform similarly for \gls{FFFS} and \gls{APFS}, where \gls{APFS} is in general faster than \gls{FFFS}. However, for certain data points, such as \texttt{file size = 131072kB, buffer size = 256kB}, \gls{FFFS} has higher performance than \gls{APFS} with \SI[per-mode = symbol]{7\,525\,973}{\kilo\byte\per\second} for \gls{FFFS} and \SI[per-mode = symbol]{5\,927\,107}{\kilo\byte\per\second} for \gls{APFS}. As the file size is bigger than \SI{5}{\mega\byte} it was not stored in the cache of \gls{FFFS} and the filesystem therefore had to read it from \gls{APFS}. The reason why \gls{FFFS} outperformed \gls{APFS} at this data point is therefore most likely due to process scheduling or because the buffer size is less efficient for \gls{APFS} than the one called on \gls{APFS} by \gls{FFFS}. The buffer size used by \gls{FFFS} to read the file from \gls{APFS} is not certain to be the same as \gls{FFFS} was called with. The buffer size used by \gls{FFFS} on \gls{APFS} is set by the implementation of \texttt{basic\_filebuf}\,\cite{cppreference.comStdBasicFilebuf2020}.

The cache of the filesystems can greatly influence the performance of the read operation. However, there is no significant drop in performance for \gls{FFFS} when reading a file that fits in the \gls{FFFS} cache, and one that does not. In fact, the performance of the Read test for \texttt{file size = 8192kB} is in general better than for \texttt{file size = 4096kB}, even though the \SI{8192}{\kilo\byte} file cannot fit in the \gls{FFFS} cache. However, the performance does drop significantly for \texttt{file size = 262144kB} compared to the previous file sizes. The reason to this is most likely that it has to save the data as two images rather than one as the image size limit of \gls{FFFS} is the same as the limit for \gls{FFS}. All files that are read in \gls{FFFS} that are not in the \gls{FFFS} cache are read from disk, which invokes at least one \gls{APFS} read operation. While the \gls{APFS} read operation called might not be called with the same buffer size as the read operation called by IOZone on \gls{FFFS}, the performance of the \gls{FFFS} read operation cannot exceed the \gls{APFS} read operation. However, the similarity of the performance between \gls{FFFS} and \gls{APFS} indicates that \gls{FFS} implements fast read operations, and that the read operation performance of \gls{FFS} depends to a great extent on the internet bandwidth and latency to the \gls{OWS}, as well as the \gls{OWS}'s data processing performance.

The only implementation difference between \gls{FFS} and \gls{FFFS} is that \gls{FFS} stores the produced images on Flickr while \gls{FFFS} stores the produced images on the local filesystem. Therefore, the time difference of a \gls{FFS} operation compared to a \gls{FFFS} operation should only depend on the internet connection to Flickr and how fast Flickr can process the requests. For instance, looking at the Write tests for the two filesystems, \gls{FFFS} outperforms \gls{FFS} significantly. Looking at one file size, for instance, \texttt{file size = 8192kB} \gls{FFFS} has an average performance of \SI[per-mode = symbol]{250\,000}{\kilo\byte\per\second} and \gls{FFS} has an average performance of \SI[per-mode = symbol]{11\,600}{\kilo\byte\per\second}. This means that the test took on average \SI{33}{\milli\second} for \gls{FFFS} and \SI{706}{\milli\second} for \gls{FFS}, meaning that the requests and the request's overhead by \gls{FFS} took on average \SI{673}{\milli\second} which is 95\% of the computation time for this test. This indicates that with a faster internet connection to Flickr, \gls{FFS} could potentially be faster. However, it is possible that the internet connection to Flickr is the fastest Flickr can handle. \gls{FFS} could then be improved by switching \gls{OWS}.

While the values of the read operation for \gls{FFFS} and \gls{APFS} are comparable to each other, this is not the case for all tests. For instance the write operation of \gls{FFFS} is much slower than the write operation of \gls{APFS}, as can be seen in Table~\ref{tbl:data_fejk-ffs_write} and Table~\ref{tbl:data_local_write}. The write operation performance average of \gls{FFFS} is about 1.5\% of the average performance of the write operation on \gls{APFS}. The reason for this could be the fact that \gls{FFFS} has to encrypt the data stored, including creating all the cryptographic variables such as the salt and the \gls{IV}. While \gls{APFS} is also an encrypted filesystem, it is possible that the filesystem prepares the cryptographic variables before they are needed. For instance, the next cryptographic key and its salt could be derived while the filesystem is idle as it does not depend on the content of the stored data. It is also possible that \gls{APFS} is using multiple threads to write the data which could speed up the operation. For instance, it is possible that \gls{FFFS} requires multiple \texttt{write} calls to the underlying \gls{APFS} filesystem. If each \texttt{write} call to \gls{APFS} uses a different thread, the multiple \texttt{write} calls could possibly be completed faster rather than if they were completed sequentially. 

\gls{FFFS} and \gls{GCSF} are comparable in some tests, which is interesting as \gls{GCSF} is dependent on an internet connection while \gls{FFFS} is not. The median performance of the \mbox{Re-Read} test on \gls{GCSF} is slightly worse than the medium performance of the \mbox{Re-Read} test on \gls{FFFS}. Meanwhile, the median Read performance of \gls{GCSF} is significantly less than the median Read performance of \gls{FFFS}. This indicates that \gls{GCSF} implements a fast cache. However, the data of \gls{GCSF} does not include the \SI{262144}{\kilo\byte} file size. The Write, \mbox{Re-Write}, and Random Write tests on \gls{FFFS} outperform the same tests on \gls{GCSF}. This is reasonable as the data written to \gls{GCSF} must be uploaded to Google Drive, while the data written to \gls{FFFS} is stored on the local disk. Uploading \SI{16}{\mega\byte} of data with the average (reference point) upload speed of \SI[per-mode = symbol]{91.83}{\mega\bit\per\second} would take about \SI{1.4}{\second}. Meanwhile, we can see in Figure~\ref{fig:bench_apfs_write} that \gls{APFS} can write \SI{16}{\mega\byte} of data as fast as \SI[per-mode = symbol]{6921222}{\kilo\byte\per\second} = \SI[per-mode = symbol]{6921.2}{\mega\byte\per\second}, meaning it would take about \SI{2}{\milli\second} to write the data. Meanwhile, the maximum Write performance of \gls{FFFS} is \SI[per-mode = symbol]{101677}{\kilo\byte\per\second} according to Figure~\ref{fig:bench_fffs_write}, meaning that \gls{FFFS} can write \SI{16}{\mega\byte} in about \SI{157}{\milli\second}. With this data, we can see that \gls{FFFS} can write the \SI{16}{\mega\byte} of data about 7800\% slower than \gls{APFS} can, and that \gls{GCSF} can write the \SI{16}{\mega\byte} of data about 800\% slower than \gls{FFFS}. 

It is easy to see, and it is not unexpected, that \gls{APFS} outperforms \gls{FFS} in performance. As a professional local filesystem, \gls{APFS} will always have better performance than FFS. Further, like \gls{FFFS}, the performance of \gls{FFS} depends on the performance of \gls{APFS} as the file which is uploaded to Flickr first needs to be saved on disk. This dependency could be removed, for instance by providing the temporary file to the FlickCURL library via a \gls{FUSE} filesystem. Further, the median performance of the \mbox{Re-Read} test on \gls{FFS} is about 72\% of the performance of the \mbox{Re-Read} test on \gls{APFS}. With higher bandwidth and with another \gls{OWS}, it is possible that \gls{FFS} could increase its performance. In contrast, the median performance of the \mbox{Re-Read} test on \gls{FFS} is about 76\% of the median performance of the same test on \gls{GCSF}.

% TODO: 
% - Analyze the time of "overhead", i.e. time of FFFS - time of APFS
%		Does not really make sense though as the buffer size to APFS is unknown, meaning it's hard
%		to know which buffer size to compare to. The highest performing? Maybe the lowest?