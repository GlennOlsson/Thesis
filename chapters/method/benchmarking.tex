\section{Benchmarking}
This section describes the methodology and execution of the different filesystem benchmarks. Two different filesystems that are relevant to \gls{FFS}: (1) \gls{APFS} and (2) \gls{GCSF}, are compared with the result of two different instances of \gls{FFS}: (1) one instance that uses Flickr as its \gls{OWS}, and (2) one instance that uses a \gls{FOWS} by storing the encoded images in the local filesystem on the test machine. 

\subsection{Filesystems}
To analyze the performance of \gls{FFS}, a filesystem benchmarking tool was used to compare \gls{FFS} against other filesystems that are relevant to \gls{FFS}. The filesystems \gls{FFS} is compared to are:
\needspace{4\baselineskip}
\begin{enumerate}
	\item An encrypted \gls{APFS} partition on an SSD,
	\item An instance of \gls{GCSF}, and,
	\item An instance of \gls{FFFS} using an encrypted \gls{APFS} filesystem on an SSD as its \gls{FOWS}.
\end{enumerate}
The encrypted \gls{APFS} filesystem was used as a reference for a local filesystem without an internet connection. An encrypted \gls{APFS} is the local filesystem of the development environment for \gls{FFS}, it was selected as an example of a modern, \mbox{well-used}, and fast filesystem, and provide a baseline benchmark to which \gls{FFS} and the other filesystems compared.

\gls{GCSF} was selected to compare \gls{FFS} against another \mbox{network-based} filesystem. While \gls{GCSF} is not a deniable filesystem, it is a filesystem that stores its data on an \gls{OWS}, namely Google Drive. The reason \gls{GCSF} was used instead of, for instance, the official Google Drive mountable filesystem volume provided by the Google Drive Desktop application, is that \gls{GCSF} provides instant upload of the files and directories to Google Drive using the Google Drive REST \gls{API}. The instant upload provided by \gls{GCSF} enables us to easily measure the duration of a file operation. For instance, a write operation on a file in \gls{GCSF} will not complete before the new file data has been completely stored on Google Drive. Another reason why \gls{GCSF} was chosen is because it is a recent filesystem compared to other related filesystems. Some of the other filesystems discussed in Section~\ref{sec:rel_fs} were developed many years before \gls{FFS} and thus no longer work as expected, for instance, due to changes in the \gls{API}, or because the \gls{OWS} manages the uploaded data differently than previously.

The instance of \gls{FFFS} using a \gls{FOWS} of an encrypted \gls{APFS} was chosen to be compared to \gls{FFS} so that the duration of the \gls{FUSE} filesystem operations could be further analyzed. As the filesystem operations of \gls{FFFS} are similar to the ones of \gls{FFS}, other than the network request being replaced by local filesystem operations, it is possible to analyze the effect of the \gls{OWS} latency, the \gls{OWS} internet connection bandwidth and latency, and the \gls{OWS} data processing speed has on the filesystem performance. Comparing the benchmark results of \gls{FFFS} and \gls{APFS} allows us to analyze the \gls{FFS} overhead as \gls{FFFS} is dependent on the performance of \gls{APFS}. Especially for file operations where \gls{FFFS} must interact with the storage medium, for instance, write operations and read operations for files not in the cache, \gls{FFFS} cannot outperform \gls{APFS} as \gls{FFFS} requires the execution time of the underlying \gls{APFS} file operation as well as the internal \gls{FFS} computation time. Both \gls{FFFS} and \gls{FFS} were mounted with a maximum buffer size of \SI{32}{\mega\byte}.

\subsection{IOZone}
IOZone\,\cite{IozoneFilesystemBenchmark2016} is a filesystem benchmarking tool used to analyze the performance of filesystem file operations using different tests on a file\,\cite{iozoneIozoneFilesystemBenchmark}. Examples of tests that IOZone provides support for are: reading and writing, reading and writing randomly, and reading backward. Each test can be run with different file sizes and different buffer sizes for the read or write operation. Normally, multiple buffer sizes are used for each test and for each file size tested. The buffer size starts at \SI{4}{\kilo\byte} and increases by a multiple of two up to a buffer size equal to the file size. Multiple file sizes are often used for benchmarking tests as well, which are also increased by a multiple of two. For instance, one could run the IOZone tests with file size \SI{1\,024}{\kilo\byte} and \SI{2\,048}{\kilo\byte}, which would utilize the following values of the file size and buffer size for each test specified:
\needspace{18\baselineskip}
\begin{enumerate}
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{16}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{32}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{64}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{128}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{256}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{512}{\kilo\byte},
	\item File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{1\,024}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{16}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{32}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{64}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{128}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{256}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{512}{\kilo\byte},
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{1\,024}{\kilo\byte}, and
	\item File size = \SI{2\,048}{\kilo\byte}, buffer size = \SI{2\,048}{\kilo\byte}.
\end{enumerate}
Furthermore, each test is run for each file size-buffer size pair before the buffer size or file size is increased. This means that, for the example above using the tests: Read, Write, Re-Read, Re-Write, Random Read, and Random Write, IOZone will run the following in order:
\needspace{9\baselineskip}
\begin{enumerate}
	\item Write test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item \mbox{Re-Write} test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Read test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item \mbox{Re-Read} test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Random Read test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Random Write test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Write test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte},
	\item \mbox{Re-Write} test for: File size = \SI{1\,024}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte}, et cetera.
\end{enumerate}

When IOZone reads from a file it has written to, it checks that the file content is what it wrote previously to verify that the filesystem stores the data properly. This is not documented in the IOZone documentation\,\cite{iozoneIozoneFilesystemBenchmark} but was discovered during testing. However, while it checks that file operations function correctly, it does not verify all aspects of the filesystem functionality. Further, as IOZone does not state that the file operations are tested, it cannot be assumed that the file operations are correct. Additionally, IOZone does not test if directory hierarchies work as expected, nor if multiple files can be stored at the same time. IOZone is a benchmarking tool used for evaluating the performance of the file operations of a filesystem, not testing the functionality of a filesystem. However, certain cases of the functionality of both \gls{FFS} and \gls{GCSF} were tested in order to see that they support directory hierarchies and multiple files as expected. Future work includes analyzing the functionality of \gls{FFS} and \gls{GCSF}. \gls{APFS} is expected to have full functionality as it is a professionally developed and widely used filesystem.

While IOZone supports multiple different file operation tests, this thesis only used a subset of these for benchmarking. Among other reasons, certain tests failed when ran on \gls{GCSF}. Furthermore, tests such as backward reading lack relevance as it tests a rare case of filesystem operations. The documentation of IOZone\,\cite{iozoneIozoneFilesystemBenchmark} claims that the software MSC Nastran uses \mbox{backward-read}. The documentation also mentions that only a few operating systems provide enhancements for backward reading, although many operating systems provide enhancements for \mbox{forward-reading}. As \gls{FFS} is intended as a \mbox{proof-of-concept} filesystem and is not intended as a \mbox{general-purpose} filesystem, only relevant tests were chosen. The IOZone benchmarking tests used in the thesis are: Forward- Read and Write, Forward- \mbox{Re-Read} and \mbox{Re-Write}, and Random- Read and Write. The \textit{Forward} specifier will sometimes be omitted in the thesis when the tests are referenced. For instance, when mentioning the Read test, we refer to the Forward Read test. The Forward Write test is writing data to a new file, \mbox{in order}. The file is first created which can include overhead time. Forward Read tests reading the file in order. The Read test tests reading from the same file \mbox{in-order}. The \mbox{Re-Write} test measures the performance of writing to an existing file \mbox{in-order}. In general, \mbox{Re-Write} is faster than Write as it does not have to create the file. \mbox{Re-Read} measures the performance of reading a recently read file \mbox{in-order}. In general, \mbox{Re-Read} is faster than \mbox{Read} as the file data can be kept in any of the caches. The Random Write and Random Read tests measure writing and reading data from random positions in a file. Depending on filesystem implementation, this can be influenced by many factors. For instance, as \gls{FFS} has to download and read the entire file into memory before accessing a specific offset, \gls{FFS} might provide lower performance for these tests than a filesystem that can choose to only read a part of a file at a specific location. The official descriptions of these tests can be found in the IOZone documentation\,\cite{iozoneIozoneFilesystemBenchmark}.

The IOZone documentation\,\cite{iozoneIozoneFilesystemBenchmark} states that to get the most accurate performance results from the benchmarking, the maximum file size of the tests should be set to a value bigger than the filesystem cache. While the \gls{FFS} cache limit is known to be \SI{5}{\mega\byte}, the cache size limit or the existence of such a limit for the other filesystems, such as \gls{GCSF}\footnote{Size limit of the cache entries of \gls{GCSF} cannot be configured. However, the number of cache entries and the \mbox{time-to-live} per cache entry can be configured}, is unknown. The IOZone documentation states that when the cache is unknown, it should be set to be greater than the physical memory of the system. The memory of the computer where the benchmarking is run is \SI{16}{\giga\byte} which is a greater value than reasonable for testing \gls{FFS} and \gls{GCSF} due to the execution time of the tests. Each doubled file size takes exponentially more time as both the file size and buffer size are doubled for each test. Furthermore, it was found that \gls{GCSF} will crash for large file sizes (as occurred for file sizes equal to or greater than \SI[per-mode = symbol]{262\,144}{\kilo\byte}). The maximum file size for the benchmarking tests was set as \SI{262\,144}{\kilo\byte} as this is bigger than the biggest image possible to store on Flickr, requiring \gls{FFS} to split the image into more than one image on the \gls{OWS}. This helped us test the functionality of FFS further. The file sizes used for the IOZone tests were therefore set as:
\needspace{10\baselineskip}
\begin{enumerate}
	\item \SI{1\,024}{\kilo\byte},
	\item \SI{2\,048}{\kilo\byte},
	\item \SI{4\,096}{\kilo\byte},
	\item \SI{8\,192}{\kilo\byte},
	\item \SI{16\,384}{\kilo\byte},
	\item \SI{32\,768}{\kilo\byte},
	\item \SI{65\,536}{\kilo\byte},
	\item \SI{131\,072}{\kilo\byte}, and
	\item \SI{262\,144}{\kilo\byte}
\end{enumerate}
The biggest buffer size IOZone uses is \SI{16\,384}{\kilo\byte}, therefore the buffer sizes tested were:
\needspace{13\baselineskip}
\begin{enumerate}
	\item \SI{4}{\kilo\byte},
	\item \SI{8}{\kilo\byte},
	\item \SI{16}{\kilo\byte},
	\item \SI{32}{\kilo\byte},
	\item \SI{64}{\kilo\byte},
	\item \SI{128}{\kilo\byte},
	\item \SI{256}{\kilo\byte},
	\item \SI{512}{\kilo\byte},
	\item \SI{1\,024}{\kilo\byte},
	\item \SI{2\,048}{\kilo\byte},
	\item \SI{4\,096}{\kilo\byte},
	\item \SI{8\,192}{\kilo\byte}, and
	\item \SI{16\,384}{\kilo\byte}
\end{enumerate}
However, the actual maximum buffer size for each file size is limited to the file size itself. For instance, for a file size of \SI{4\,096}{\kilo\byte}, IOZone will run the tests for buffer sizes up to, and including, \SI{4\,096}{\kilo\byte}. It does not make sense to run tests with a buffer size greater than \SI{4\,096}{\kilo\byte} for a file size of \SI{4\,096}{\kilo\byte}.

While \gls{GCSF} does not define a maximum file size for its cache, one can configure the \mbox{time-to-live} of the cache entries, and the number of cache entries allowed in the cache\,\cite{sergiudanGcsfConfigRust}. The \gls{GCSF} cache is stored \mbox{in-memory} so the cache will be cleared if the \gls{GCSF} process is terminated. During benchmarking, The \gls{GCSF} cache was configured to store up to 20 cache entries for up to 300 seconds per cache entry. The cache entry limit is the same limit as the cache entry limit of \gls{FFS}. 300 seconds \mbox{time-to-live} limit was set for the entries is it was the default configuration for the \mbox{time-to-live}.

IOZone reads and writes the same file. The file is created and removed during the tests, but there are never two files created by IOzone at the same time. When conducting the benchmarking for this thesis, the file is stored in the root directory of \gls{FFS}, \gls{FFFS}, and \gls{GCSF}. This means the path traversal is as short as possible for these filesystems. Furthermore, as \gls{FFS} and \gls{FFFS} constantly cache the root directory and both have a cache limit of 20 entries, and as no other operations are performed on the filesystem at the same time as the benchmarking tests, the file can always be stored in the cache as long as the file is small enough. The cache entry of the benchmarking file will not be removed unless the file is removed as the cache will not be filled by other entries. The file used for benchmarking \gls{APFS} was in temporary directory in the root directory of the computer (\texttt{/tmp}). The reason is because the root directory of the \gls{APFS} on the computer was a \mbox{Read-only} filesystem. Furthermore, \gls{FFFS} saves its files in the same temporary directory, so the performance of saving the images for \gls{FFFS} and the performance of the \gls{APFS} write operation can be compared. Future work could compare benchmarking \gls{FFS}, \gls{FFFS}, \gls{GCSF}, and \gls{APFS} in different depths of directory hierarchies to analyze how the depth of a file impact the performance of the filesystem.

When benchmarking the filesystems using IOZone, an argument is passed to include the time to close a file (using the \texttt{close} filesystem operation) in the total time of a test. This is important as \gls{FFS}, and potentially other filesystems, save the data to the storage medium only after the device is closed. In the case of \gls{FFS}, if the time of closing the file was not included, the performance of the filesystem would appear to be higher than it is.

IOZone produces a log of the benchmarking results for the filesystem it benchmarked. This log contains a report of each test of each file operation with performance data for each file size and each buffer size used in the benchmark. The performance of the filesystem is measured in kilobytes per second. For this thesis, each filesystem is benchmarked 20 times with the kernel cache (\gls{UBC}) enabled and 20 times with the kernel cache disabled. By comparing the benchmarks were the kernel cache is enabled and disabled, we expect to see better performance from the benchmarks were the filesystem has the kernel cache enabled. During testing, before running the experiments, it was found that one IOZone benchmark takes around three to five hours for \gls{FFS} and \gls{GCSF}. Therefore, a much large number of tests per filesystem would take very long time to complete. When benchmarking \gls{FFS}, \gls{GCSF}, and \gls{FFFS}, the processes running the filesystems were terminated and the filesystems were unmounted and \mbox{re-mounted} between each iteration to clear their internal caches.

The benchmarking tests the filesystems were carried out in Amsterdam in The Netherlands using an ethernet connection to a \mbox{fiber-connected} router in a resedental home with a bandwidth subscription of \SI{100}{\mega\bit} download speed and \SI{20}{\mega\bit} upload speed. The WiFi of the computer was disabled during benchmarking so all network traffic was sent through the ethernet interface the ethernet cable was connected to. The filesystem benchmarks were conducted during all hours of the day, during both week days and weekends. Other devices connected to the same network were not using a significant amount of internet bandwidth during this time to avoid affecting the available bandwidth for the computer running the tests. The computer on which the filesystem benchmarks were run was running as few processes as possible during the benchmarks to avoid other processes competing for resources of the computer, such as internet bandwidth and memory. For instance, the web browser was never running during any of the benchmarks. These processes were manually killed before the benchmarks started.

During the benchmarking of \gls{FFS} and \gls{GCSF}, the bandwidth used by the computer was measured using Wireshark\,\cite{WiresharkGoDeep}. Wireshark was run in the command line instead of in the graphical user interface to avoid Wireshark using more resources of the computer than necessary. The bandwidth analyzer was configured to capture all outgoing and incoming packets using TCP on port 80 or 443 (at either the source or the destination) over the ethernet interface the ethernet cable is connected to, during the benchmarking process. Port 80 and 443 are normally used for HTTP and HTTPS requests, respectively, which is how both \gls{FFS} and \gls{GCSF} communicate with their storage services. However, Wireshark captured all packets sent and received by the computer, not only the packets related to the filesystems. When researching tools that could measure the bandwidth over time, per application, no such tools were found that could be run on macOS. While the computer were running as few background processes as possible, some processes were found to sometimes start again after they have been killed by the user. This means that the result from Wireshark could include packets associated with other processes that are sending and receiving HTTP packets. In future work, it could be valuable to find a way to isolate the bandwidth measurement to only capture packets associated with the process. Wireshark reported the number of packets and bytes per ten-second interval to a file after the benchmarking was completed. 

Wireshark consumes a lot of memory to capture all packets sent and received by the computer, even when the filtering is applied. Due to the long benchmarking execution time of \gls{FFS} and \gls{GCSF}, Wireshark was configured to save its measurements in temporary files during the benchmarking so the computer would not exceed its memory limit. Each temporary file was configured to be a maximum of \SI{1}{\giga\byte} before a new temporary file was used. These files were summarized when the Wireshark measurement was ended. To avoid Wireshark competing for filesystems operations on \gls{APFS}, these temporary files were saved on an external hard drive.

% TODO: Can we replace with wireshark instead? Filter on tcp and port 80 (to and from)
%	- However, testing does not show same bandwidth for BBK and wireshark unless filtering by time span
%	- Does this work well with ffs, gcsf etc? What is correct - wireshark or bbk?