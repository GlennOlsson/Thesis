\section{Benchmarking}
\label{sec:benchmarking}
This section describes the methodology and execution of the filesystem benchmarks for a number of different filesystems. Two filesystems that are relevant to \gls{FFS} are \first \gls{APFS} and \Second \gls{GCSF}. The benchmarking of these two filesystems are compared with the result of two different instances of \gls{FFS}: \first one instance that uses Flickr as its \gls{OWS}, and \Second one instance that uses a \gls{FOWS} that stores the encoded images in the local filesystem on the test machine. 

\subsection{Filesystems}
To analyze the performance of \gls{FFS}, the IOZone filesystem benchmarking tool described in \Cref{sec:iozoneDetails} was used to compare \gls{FFS} using Flickr as its \gls{OWS} against the following filesystems:
\needspace{4\baselineskip}
\begin{enumerate}
	\item An encrypted \gls{APFS} partition on an SSD,
	\item An instance of \gls{GCSF}, and,
	\item An instance of \gls{FFFS} using an encrypted \gls{APFS} filesystem on an SSD as its \gls{FOWS}.
\end{enumerate}
The encrypted \gls{APFS} filesystem was used as a reference for a local filesystem without an internet connection. An encrypted \gls{APFS} is the local filesystem of the development environment for \gls{FFS}. \gls{APFS} was selected as an example of a modern, \mbox{well-used}, and fast filesystem, and it provides a baseline benchmark to which \gls{FFS} and the other filesystems can be compared.

\gls{GCSF} was selected to compare \gls{FFS} against another \mbox{network-based} filesystem. While \gls{GCSF} is not a deniable filesystem, it is a filesystem that stores its data on an \gls{OWS}, namely Google Drive. The reason \gls{GCSF} was used instead of, for instance, the official Google Drive mountable filesystem volume provided by the Google Drive Desktop application, is that \gls{GCSF} provides instant upload of the files and directories to Google Drive using the Google Drive REST \gls{API}. The instant upload provided by \gls{GCSF} enables us to easily measure the duration of a file operation. For instance, a write operation on a file in \gls{GCSF} will not complete before the new file data has been completely stored on Google Drive. Another reason why \gls{GCSF} was chosen is because it is a recent filesystem compared to other related filesystems. Some of the other filesystems discussed in~\Cref{sec:rel_fs} were developed many years before \gls{FFS} and thus no longer work as expected, for instance, due to changes in the \gls{API} or because the \gls{OWS} manages the uploaded data differently than it did previously.

The instance of \gls{FFFS} using a \gls{FOWS} of an encrypted \gls{APFS} was chosen so that the duration of the \gls{FUSE} filesystem operations could be further analyzed. As the filesystem operations of \gls{FFFS} are similar to the ones of \gls{FFS}, other than the network request being replaced by local filesystem operations, it is possible to analyze the effect of the \gls{OWS} latency, the \gls{OWS} internet connection bandwidth and latency, and the \gls{OWS} data processing speed on the filesystem's performance. Comparing the benchmark results of \gls{FFFS} and \gls{APFS} allows us to analyze the \gls{FFS} overhead as \gls{FFFS} is dependent on the performance of \gls{APFS}. Especially for file operations where \gls{FFFS} must interact with the storage medium, for instance, write operations and read operations for files not in the cache, \gls{FFFS} cannot outperform \gls{APFS} as \gls{FFFS} requires the execution of the underlying \gls{APFS} file operation as well as the internal \gls{FFS} computation time. Both \gls{FFFS} and \gls{FFS} were mounted with a maximum buffer size of \SI{32}{\mega\byte}.

\subsection{IOZone}
\label{sec:iozoneDetails}
IOZone\,\cite{IozoneFilesystemBenchmark2016} is a filesystem benchmarking tool used to analyze the performance of filesystem file operations using different tests on a file\,\cite{iozoneIozoneFilesystemBenchmark}. Examples of tests that IOZone provides support for are: reading and writing, reading and writing randomly, and reading backward. Each test can be run with different file sizes and different buffer sizes for the read or write operation. Normally, multiple buffer sizes are used for each test and for each file size tested. The buffer size starts at \SI{4}{\kilo\byte} and increases by a multiple of two up to a buffer size equal to the file size. Multiple file sizes are often used for benchmarking tests as well, these sizes are also increased by a multiple of two. For instance, running the IOZone tests with file size \SI{1024}{\kilo\byte} and \SI{2048}{\kilo\byte}, would utilize the following values of the file size and buffer size for each test specified:
\needspace{18\baselineskip}
\begin{enumerate}
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{16}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{32}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{64}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{128}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{256}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{512}{\kilo\byte},
	\item File size = \SI{1024}{\kilo\byte}, buffer size = \SI{1\,024}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{16}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{32}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{64}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{128}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{256}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{512}{\kilo\byte},
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{1024}{\kilo\byte}, and
	\item File size = \SI{2048}{\kilo\byte}, buffer size = \SI{2048}{\kilo\byte}.
\end{enumerate}
Furthermore, each test is run for each file size-buffer size pair before the buffer size or file size is increased. This means that, for the example above using the tests: Read, Write, Re-Read, Re-Write, Random Read, and Random Write, IOZone will run the tests in the following order:
%\needspace{8\baselineskip}
\begin{enumerate}
	\item Write test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item \mbox{Re-Write} test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Read test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item \mbox{Re-Read} test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Random Read test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Random Write test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{4}{\kilo\byte},
	\item Write test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte},
	\item \mbox{Re-Write} test for: File size = \SI{1024}{\kilo\byte}, buffer size = \SI{8}{\kilo\byte}, \etc.
\end{enumerate}

When IOZone reads from a file it has written to, it checks that the file content is what it wrote previously to verify that the filesystem stores the data properly. This is \textbf{not} documented in the IOZone documentation\,\cite{iozoneIozoneFilesystemBenchmark} but was discovered during testing. However, while it checks that file operations function correctly, it does \textbf{not} verify all aspects of the filesystem functionality. Further, as IOZone does \textbf{not} state that the file operations are tested, it cannot be assumed that the file operations are correct. Additionally, IOZone does \textbf{not} test if directory hierarchies work as expected, nor if multiple files can be stored at the same time. This means that IOZone is a benchmarking tool for evaluating the \textit{performance of the file operations of a filesystem and not testing the functionality of a filesystem}.

While IOZone does not test the full functionality of a filesystem, certain cases of the functionality of both \gls{FFS} and \gls{GCSF} were tested to see that they support directory hierarchies and multiple files as expected. Future work includes analyzing the functionality of \gls{FFS} and \gls{GCSF}. \gls{APFS} is expected to have full functionality as it is a professionally developed and widely used filesystem.

While IOZone supports multiple different file operation tests, this thesis only used a subset of these for benchmarking. Among other reasons, certain tests failed when ran on \gls{GCSF}. Furthermore, tests such as backward reading lack relevance as it tests a rare case of filesystem operations. \footnote{The documentation of IOZone\,\cite{iozoneIozoneFilesystemBenchmark} claims that the software MSC Nastran uses \mbox{backward-read}. The documentation also mentions that only a few operating systems provide enhancements for backward reading, although many operating systems provide enhancements for \mbox{forward-reading}.} As \gls{FFS} is intended as a \mbox{proof-of-concept} filesystem and is not intended as a \mbox{general-purpose} filesystem, only relevant tests were chosen. The IOZone benchmarking tests used in the thesis are: Forward- Read and Write, Forward- \mbox{Re-Read} and \mbox{Re-Write}, and Random- Read and Write. The \textit{Forward} specifier will sometimes be omitted in the thesis when the tests are referenced. For instance, when mentioning the Read test, this actually refers to the Forward Read test. The Forward Write test is writing data to a new file, \mbox{in order}. The file is first created which can include overhead time. Forward Read tests read the file in order. The Read test tests reading from the same file \mbox{in-order}. The \mbox{Re-Write} test measures the performance of writing to an existing file \mbox{in-order}. In general, \mbox{Re-Write} is faster than Write as it does not have to create the file. \mbox{Re-Read} measures the performance of reading a recently read file \mbox{in-order}. In general, \mbox{Re-Read} is faster than \mbox{Read} as the file data can be kept in any of the caches. The Random Write and Random Read tests measure writing and reading data from random positions in a file. Depending on filesystem implementation, this can be influenced by many factors. For instance, as \gls{FFS} has to download and read the entire file into memory before accessing a specific offset, \gls{FFS} might provide lower performance for these tests than a filesystem that can choose to read only a part of a file at a specific location. The official descriptions of these tests can be found in the IOZone documentation\,\cite{iozoneIozoneFilesystemBenchmark}.

The IOZone documentation\,\cite{iozoneIozoneFilesystemBenchmark} states that to get the most accurate performance results from the benchmarking, the maximum file size of the tests should be set to a value bigger than the filesystem cache. While the \gls{FFS} cache limit is known to be \SI{5}{\mega\byte} per file, the cache size limit or the existence of such a limit for the other filesystems, such as \gls{GCSF}\footnote{The size limit of the cache entries of \gls{GCSF} cannot be configured and is unknown. However, the number of cache entries and the \mbox{time-to-live} per cache entry can be configured.}. The IOZone documentation states that when the cache is unknown, he maximum file size of the tests should be set to be greater than the physical memory of the system. The memory of the computer where the benchmarking is run is \SI{16}{\giga\byte} which is a greater value than reasonable for testing \gls{FFS} and \gls{GCSF} due to the execution time of the tests. Each doubled file size takes exponentially more time as both the file size and buffer size are doubled for each test. Furthermore, it was found that \gls{GCSF} crashed for large file sizes (\ie file sizes equal to or greater than \SI[per-mode = symbol]{262144}{\kilo\byte}). As a result, the maximum file size for the benchmarking tests was set as \SI{262144}{\kilo\byte} as this was bigger than the biggest image possible to store on Flickr, requiring \gls{FFS} to split the image into more than one image on the \gls{OWS}. This helped to further test the functionality of FFS. The file sizes used for the IOZone tests were therefore set as:
\needspace{10\baselineskip}
\begin{enumerate}
	\item \SI{1024}{\kilo\byte},
	\item \SI{2048}{\kilo\byte},
	\item \SI{4096}{\kilo\byte},
	\item \SI{8192}{\kilo\byte},
	\item \SI{16384}{\kilo\byte},
	\item \SI{32768}{\kilo\byte},
	\item \SI{65536}{\kilo\byte},
	\item \SI{131072}{\kilo\byte}, and
	\item \SI{262144}{\kilo\byte}
\end{enumerate}
The biggest buffer size IOZone uses is \SI{16384}{\kilo\byte}, therefore the buffer sizes tested were:
\needspace{13\baselineskip}
\begin{enumerate}
	\item \SI{4}{\kilo\byte},
	\item \SI{8}{\kilo\byte},
	\item \SI{16}{\kilo\byte},
	\item \SI{32}{\kilo\byte},
	\item \SI{64}{\kilo\byte},
	\item \SI{128}{\kilo\byte},
	\item \SI{256}{\kilo\byte},
	\item \SI{512}{\kilo\byte},
	\item \SI{1024}{\kilo\byte},
	\item \SI{2048}{\kilo\byte},
	\item \SI{4096}{\kilo\byte},
	\item \SI{8192}{\kilo\byte}, and
	\item \SI{16384}{\kilo\byte}
\end{enumerate}

However, the maximum buffer size for each file size is limited to the file size itself. For instance, for a file size of \SI{4096}{\kilo\byte}, IOZone will run the tests for buffer sizes up to and including \SI{4096}{\kilo\byte}. This is done as it does not make sense to run tests with a buffer size greater than the file size.

As mentioned above, while \gls{GCSF} does not define a maximum file size for its cache, one can configure the \mbox{time-to-live} of the cache entries, and the number of cache entries allowed in the cache\,\cite{sergiudanGcsfConfigRust}. The \gls{GCSF} cache is stored \mbox{in-memory} so the cache will be cleared if the \gls{GCSF} process is terminated. During benchmarking, the \gls{GCSF} cache was configured to store up to 20 cache entries for up to 300 seconds per cache entry. This cache entry limit is the same as the cache entry limit of \gls{FFS}. The 300 seconds \mbox{time-to-live} limit was set for the entries as this was the default configuration for the \mbox{time-to-live}.

During testing, IOZone reads and writes the same file. This file is created and removed during the tests, but IOzone never creates two files at the same time. When conducting the benchmarking for this thesis, the file is stored in the root directory of \gls{FFS}, \gls{FFFS}, and \gls{GCSF}. This means the path traversal is as short as possible for these filesystems. Furthermore, as \gls{FFS} and \gls{FFFS} always cache the root directory. Both have a cache limit of 20 entries, and as no other operations are performed on the filesystem at the same time as the benchmarking tests, the file can always be stored in the cache as long as the file is small enough (\ie smaller than the )\SI{5}{\mega\byte}limit). The cache entry of the benchmarking file will not be removed unless the file is removed, as the cache will never be filled by other entries (hence the cache's LRU algorithm is never invoked). The file used for benchmarking \gls{APFS} was in the temporary directory in the root directory of the computer (\ie \texttt{/tmp}). The reason for this was that the root directory of the \gls{APFS} on the computer was a \mbox{Read-only} filesystem. Furthermore, \gls{FFFS} saves its files in the same temporary directory, so the performance of saving the images for \gls{FFFS} and the performance of the \gls{APFS} write operation can be compared. Future work could compare benchmarking \gls{FFS}, \gls{FFFS}, \gls{GCSF}, and \gls{APFS} at different depths of directory hierarchies to analyze how the length of the file path impacts the performance of the filesystem.

When benchmarking the filesystems using IOZone, an argument is passed to include the time to close a file (using the \texttt{close} filesystem operation) in the total time of a test. This is important as \gls{FFS}, and potentially other filesystems, save the data to the storage medium only after the device is closed. In the case of \gls{FFS}, if the time of closing the file was not included, the performance of the filesystem would appear to be higher than it actually is.

IOZone produces a log of the benchmarking results for the filesystem. This log contains a report of each test of each file operation with performance data for each file size and each buffer size used in the benchmark. The performance of the filesystem is measured in kilobytes per second. For this thesis, each filesystem was benchmarked 20 times with the kernel cache (\gls{UBC}) enabled and 20 times with the kernel cache disabled. By comparing the benchmarks where the kernel cache is enabled and disabled, we expect to see better performance from the benchmarks when the filesystem has the kernel cache enabled. During testing, before running the experiments, it was found that one IOZone benchmark takes around three to five hours for \gls{FFS} and \gls{GCSF}. Therefore, a much large number of tests per filesystem would take a very long time to complete. When benchmarking \gls{FFS}, \gls{GCSF}, and \gls{FFFS}, the processes running the filesystems were terminated, and the filesystems were unmounted and \mbox{re-mounted} between each iteration to clear their internal caches.

The benchmarking tests of the filesystems were carried out in Amsterdam in The Netherlands using an Ethernet connection to a \mbox{fiber-connected} router in a residential home with a bandwidth subscription of \SI{100}{\mega\bit} download speed and \SI{20}{\mega\bit} upload speed. The WiFi interface of the computer was disabled during benchmarking so all network traffic was sent through the Ethernet interface that the Ethernet cable was connected to. The filesystem benchmarks were conducted during all hours of the day, during both weekdays and weekends. Other devices connected to the same network were not using a significant amount of internet bandwidth during this time to avoid affecting the available bandwidth for the computer running the tests. The computer on which the filesystem benchmarks were run was running as few processes as possible during the benchmarks to avoid other processes competing for resources of the computer, such as internet bandwidth and memory. For instance, the web browser was never running during any of the benchmarks. These unnecessary processes were manually killed before the benchmarks started.

During the benchmarking of \gls{FFS} and \gls{GCSF}, the bandwidth used by the computer was measured using Wireshark\,\cite{WiresharkGoDeep}. Wireshark was run from the command line instead of via the graphical user interface to avoid Wireshark using more resources of the computer than necessary. Wireshark was configured to capture all outgoing and incoming packets using TCP on port 80 or 443 (at either the source or the destination) over the Ethernet interface the Ethernet cable is connected to, during the benchmarking process. Port 80 and 443 are normally used for HTTP and HTTPS requests, respectively, which is how both \gls{FFS} and \gls{GCSF} communicate with their storage services. 

Wireshark captured all packets sent and received by the computer, not only the packets related to the filesystems. When researching tools that could measure the bandwidth over time per application, no such tools could be run on macOS. While the aim was to have the computer running as few background processes as possible, some processes sometimes started again after the user had killed them. This means that the result from Wireshark would include packets associated with these other processes that are sending and receiving HTTP packets. In future work, it would be valuable to find a way to isolate the bandwidth measurement only to capture packets associated with the benchmarking process. Wireshark wrote the number of packets and bytes per ten-second interval to a file \textit{after} the benchmarking was completed. 

Wireshark consumes a lot of memory to capture all packets sent and received by the computer, even when filtering is applied. Due to the long benchmarking execution time of \gls{FFS} and \gls{GCSF}, Wireshark was configured to save its measurements in temporary files during the benchmarking so the computer would not exceed its memory limit. Each temporary file was configured to be a maximum of \SI{1}{\giga\byte} before a new temporary file was used. These files were summarized when the Wireshark measurement ended. To avoid Wireshark competing for filesystems operations on \gls{APFS}, these temporary files were saved on an external hard drive.

To benchmark \gls{FFS} and \gls{GCSF}, a script was created that ran three different processes during the benchmarking:
\needspace{4\baselineskip}
\begin{itemize}
	\item One process running the filesystem process,
	\item One process running the Wireshark measurements, and
	\item One process running the IOZone benchmarking against the filesystem
\end{itemize}

For each benchmark iteration, the benchmarking script did the following:
\needspace{7\baselineskip}
\begin{itemize}
	\item Start a new process and mount the filesystem in this process,
	\item Start a new process running the Wireshark measurements,
	\item Start the IOZone benchmark,
	\item Wait until the benchmark had finished,
	\item Kill the process running the wireshark measurement and saving the output, and
	\item Kill the process running the filesystem and unmounting the filesystem.
\end{itemize}

During testing, \gls{FFS} and \gls{GCSF} were found to crash occasionally during benchmarking (due to, for instance, lost internet connectivity and undetected bugs in their implementations). The benchmarking script was built to handle these failures by killing the processes, unmounting the filesystem, and restarting the benchmark for that iteration. 

\gls{GCSF} was benchmarked on a personal Google Drive account using a \mbox{free-tier} plan. The \mbox{free-tier} plan allowed up to \SI[per-mode = symbol]{15}{\giga\byte} of storage per user. When a file is deleted from Google Drive using \gls{GCSF}, the file is moved to the trash bin where the file is kept for 30 days\,\cite{googleDeleteRestoreFiles}. As each \gls{GCSF} benchmark creates:
\begin{itemize}
	\item 9 \SI[per-mode = symbol]{1024}{\kilo\byte} files,
	\item 10 \SI[per-mode = symbol]{2048}{\kilo\byte} files,
	\item 11 \SI[per-mode = symbol]{4096}{\kilo\byte} files,
	\item 12 \SI[per-mode = symbol]{8192}{\kilo\byte} files,
	\item 13 \SI[per-mode = symbol]{16384}{\kilo\byte} files,
	\item 13 \SI[per-mode = symbol]{32768}{\kilo\byte} files,
	\item 13 \SI[per-mode = symbol]{65536}{\kilo\byte} files, and
	\item 13 \SI[per-mode = symbol]{131072}{\kilo\byte} files.
\end{itemize}
Per write test, each benchmark of \gls{GCSF} creates \SI[per-mode = symbol]{10.1}{\giga\byte} of files on Google Drive. As the trash bin is not excluded from the storage quota on Google Drive, the trash bin had to be emptied continuously during the benchmarking iterations of \gls{GCSF}. Otherwise, \gls{GCSF} crashed during the benchmarking.


% TODO: Can we replace with wireshark instead? Filter on tcp and port 80 (to and from)
%	- However, testing does not show same bandwidth for BBK and wireshark unless filtering by time span
%	- Does this work well with ffs, gcsf etc? What is correct - wireshark or bbk?